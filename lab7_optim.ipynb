{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CSC-52081-EP Lab7: Optimization - CEM / ES\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2026-students/refs/heads/main/assets/logo.jpg\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[CSC-52081-EP-2026](https://moodle.ip-paris.fr/course/view.php?id=10691) Lab session #7\n",
    "\n",
    "2019-2026 Jérémie Decock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jeremiedecock/polytechnique-csc-52081-ep-2026-students/blob/main/lab7_optim.ipynb)\n",
    "\n",
    "[![My Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jeremiedecock/polytechnique-csc-52081-ep-2026-students/main?filepath=lab7_optim.ipynb)\n",
    "\n",
    "[![NbViewer](https://raw.githubusercontent.com/jupyter/design/main/logos/Badges/nbviewer_badge.svg)](https://nbviewer.jupyter.org/github/jeremiedecock/polytechnique-csc-52081-ep-2026-students/blob/main/lab7_optim.ipynb)\n",
    "\n",
    "[![Local](https://img.shields.io/badge/Local-Save%20As...-blue)](https://github.com/jeremiedecock/polytechnique-csc-52081-ep-2026-students/raw/main/lab7_optim.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the previous lab we studied a method that allowed us to apply reinforcement learning in continuous state spaces and/or continuous action spaces.\n",
    "We used REINFORCE, a *Policy gradient* method that directly optimize the parametric policy $\\pi_{\\theta}$.\n",
    "The parameter $\\theta$ was iteratively updated toward a local maximum of the total expected reward $J(\\theta)$ using a gradient ascent method:\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\nabla_{\\theta}J(\\theta)$$\n",
    "A convenient analytical formulation of $\\nabla_{\\theta}J(\\theta)$ was obtained thanks to the *Policy Gradient theorem*:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\nabla_\\theta V^{\\pi_\\theta}(s) = \\mathbb{E}_{\\pi_\\theta} \\left[\\nabla_\\theta \\log \\pi_\\theta (a|s) Q^{\\pi_\\theta}(s,a) \\right].$$\n",
    "However, gradient ascent methods may have a slow convergence and will only found a local optimum.\n",
    "Moreover, this approach requires an analytical formulation of $\\nabla_\\theta \\log \\pi_\\theta (s,a)$ which is not always known (when something else than a neural networks is used for the agent's policy).\n",
    "\n",
    "Direct Policy Search methods using gradient free optimization procedures like Evolution Strategies or Cross Entropy Method (CEM) are interesting alternatives to Policy Gradient algorithms.\n",
    "They can be successfully applied as long as the $\\pi_\\theta$ policy has no more than few hundreds of parameters.\n",
    "Moreover, these method can solve complex problems that cannot be modeled as Markov Decision Processes.\n",
    "\n",
    "As for previous Reinforcement Learning labs, we will use standard problems provided by Gymnasium suite.\n",
    "Especially, we will try to solve the LunarLander-v3 problem (https://gymnasium.farama.org/environments/box2d/lunar_lander/) which offers both continuous states and action spaces.\n",
    "\n",
    "As for previous labs, you can either:\n",
    "- open, edit and execute the notebook in *Google Colab* following this link: https://colab.research.google.com/github/jeremiedecock/polytechnique-csc-52081-ep-2026-students/blob/main/lab7_optim.ipynb ; this is the **recommended** choice as you have nothing to install on your computer\n",
    "- open, edit and execute the notebook in *MyBinder* (if for any reason the Google Colab solution doesn't work): https://mybinder.org/v2/gh/jeremiedecock/polytechnique-csc-52081-ep-2026-students/main?filepath=lab7_optim.ipynb\n",
    "- download, edit and execute the notebook on your computer if Python3 and JupyterLab are already installed: https://github.com/jeremiedecock/polytechnique-csc-52081-ep-2026-students/raw/main/lab7_optim.ipynb\n",
    "\n",
    "If you work with Google Colab or MyBinder, **remember to save or download your work regularly or you may lose it!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
