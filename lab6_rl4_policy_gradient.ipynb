{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77910eb0",
   "metadata": {},
   "source": [
    "# CSC-52081-EP Lab6: Policy Gradient\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2026-students/refs/heads/main/assets/logo.jpg\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[CSC-52081-EP-2026](https://moodle.ip-paris.fr/course/view.php?id=10691) Lab session #6\n",
    "\n",
    "2019-2026 JÃ©rÃ©mie Decock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db40ec72",
   "metadata": {},
   "source": [
    "[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jeremiedecock/polytechnique-csc-52081-ep-2026-students/blob/main/lab6_rl4_policy_gradient.ipynb)\n",
    "\n",
    "[![My Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jeremiedecock/polytechnique-csc-52081-ep-2026-students/main?filepath=lab6_rl4_policy_gradient.ipynb)\n",
    "\n",
    "[![NbViewer](https://raw.githubusercontent.com/jupyter/design/main/logos/Badges/nbviewer_badge.svg)](https://nbviewer.jupyter.org/github/jeremiedecock/polytechnique-csc-52081-ep-2026-students/blob/main/lab6_rl4_policy_gradient.ipynb)\n",
    "\n",
    "[![Local](https://img.shields.io/badge/Local-Save%20As...-blue)](https://github.com/jeremiedecock/polytechnique-csc-52081-ep-2026-students/raw/main/lab6_rl4_policy_gradient.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c49bbc",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The aim of this lab is to provide an in-depth exploration of policy-based reinforcement learning techniques, with a particular focus on the *Monte Carlo Policy Gradient (REINFORCE)*.\n",
    "\n",
    "As for previous labs, you can either:\n",
    "- open, edit and execute the notebook in *Google Colab* following this link: https://colab.research.google.com/github/jeremiedecock/polytechnique-csc-52081-ep-2026-students/blob/main/lab6_rl4_policy_gradient.ipynb ; this is the **recommended** choice as you have nothing to install on your computer\n",
    "- open, edit and execute the notebook in *MyBinder* (if for any reason the Google Colab solution doesn't work): https://mybinder.org/v2/gh/jeremiedecock/polytechnique-csc-52081-ep-2026-students/main?filepath=lab6_rl4_policy_gradient.ipynb\n",
    "- download, edit and execute the notebook on your computer if Python3 and JupyterLab are already installed: https://github.com/jeremiedecock/polytechnique-csc-52081-ep-2026-students/raw/main/lab6_rl4_policy_gradient.ipynb\n",
    "\n",
    "If you work with Google Colab or MyBinder, **remember to save or download your work regularly or you may lose it!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8679923f",
   "metadata": {},
   "source": [
    "## Lab Submission\n",
    "\n",
    "Please submit your completed notebook in [Moodle : \"Lab 6 - Submission\"](https://moodle.ip-paris.fr/mod/assign/view.php?id=367817).\n",
    "\n",
    "### Submission Guidelines\n",
    "\n",
    "1. **File Naming:** Rename your notebook as follows: **`firstname_lastname-06.ipynb`** where `firstname` and `lastname` match your email address. *Example: `jesse_read-06.ipynb`*\n",
    "2. **Clear Output Cells:** To reduce file size (**must be under 500 KB**), clear all output cells before submitting. This includes rendered images, videos, plots, and dataframes...\n",
    "   - **JupyterLab:**\n",
    "     - Click **\"Kernel\" â†’ \"Restart Kernel and Clear Outputs of All Cells...\"**\n",
    "     - Then go to **\"File\" â†’ \"Save Notebook As...\"**\n",
    "   - **Google Colab:**\n",
    "     - Click **\"Edit\" â†’ \"Clear all outputs\"**\n",
    "     - Then go to **\"File\" â†’ \"Download\" â†’ \"Download.ipynb\"**\n",
    "   - **VSCode:**\n",
    "     - Click **\"Clear All Outputs\"**\n",
    "     - Then **save your file**\n",
    "3. **Upload Your File:** Only **`.ipynb`** files are accepted.\n",
    "\n",
    "**Note:** Bonus parts (if any) are optional, as their name suggests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1002569b",
   "metadata": {},
   "source": [
    "## Setup the Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f156171b",
   "metadata": {},
   "source": [
    "This notebook relies on several libraries including `gymnasium[classic-control]` (v1.0.0), `ipywidgets`, `matplotlib`, `moviepy`, `numpy`, `pandas`, `pygame`, `seaborn`, `torch`, and `tqdm`.\n",
    "A complete list of dependencies can be found in the following [requirements-lab6.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2026-students/main/requirements-lab6.txt) file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1077b575",
   "metadata": {},
   "source": [
    "### If you use Google Colab\n",
    "\n",
    "If you use Google Colab, execute the next cell to install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb88eee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xt66Z85AmOI2",
    "outputId": "bd7a6b75-ad3c-4be1-d560-8239fbc0e9d2"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def is_colab():\n",
    "    return \"google.colab\" in sys.modules\n",
    "\n",
    "\n",
    "def run_subprocess_command(cmd):\n",
    "    # run the command\n",
    "    process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
    "    # print the output\n",
    "    for line in process.stdout:\n",
    "        print(line.decode().strip())\n",
    "\n",
    "\n",
    "if is_colab():\n",
    "    # run_subprocess_command(\"apt install xvfb x11-utils\")\n",
    "    run_subprocess_command(\n",
    "        \"pip install -r https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2026-students/main/requirements-lab6-google-colab.txt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd689f2",
   "metadata": {},
   "source": [
    "### If you have downloaded the notebook on your computer and execute it in your own Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a224bfa2",
   "metadata": {},
   "source": [
    "To set up the necessary dependencies, run the following commands to establish a [Python virtual environment (venv)](https://docs.python.org/3/library/venv.html) that includes all the essential libraries for this lab.\n",
    "\n",
    "#### On POSIX systems (Linux, MacOSX, WSL, ...)\n",
    "\n",
    "```bash\n",
    "python3 -m venv env-lab6\n",
    "source env-lab6/bin/activate\n",
    "python3 -m pip install --upgrade pip\n",
    "python3 -m pip install -r https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2026-students/main/requirements-lab6.txt\n",
    "```\n",
    "\n",
    "#### On Windows\n",
    "\n",
    "```bash\n",
    "python3 -m venv env-lab6\n",
    "env-lab6\\Scripts\\activate.bat\n",
    "python3 -m pip install --upgrade pip\n",
    "python3 -m pip install -r https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2026-students/main/requirements-lab6.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe871d9",
   "metadata": {},
   "source": [
    "### Run CSC-52081-EP notebooks locally in a dedicated Docker container\n",
    "\n",
    "If you are familiar with Docker, an image is available on Docker Hub for this lab:\n",
    "\n",
    "```bash\n",
    "docker run -it --rm --user root -p 8888:8888 -e NB_UID=$(id -u) -e NB_GID=$(id -g) -v \"${PWD}\":/home/jovyan/work jdhp/csc-52081-ep:latest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301aa8db",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01c6df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "# from numpy.typing import NDArray\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from typing import cast, List, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2df5626",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8f318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04ebab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a1da78",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270d0a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGS_DIR = Path(\"figs/\") / \"lab6\"       # Where to save figures (.gif or .mp4 files)\n",
    "PLOTS_DIR = Path(\"figs/\") / \"lab6\"      # Where to save plots (.png or .svg files)\n",
    "MODELS_DIR = Path(\"models/\") / \"lab6\"   # Where to save models (.pth files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ae2336",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not FIGS_DIR.exists():\n",
    "    FIGS_DIR.mkdir(parents=True)\n",
    "if not PLOTS_DIR.exists():\n",
    "    PLOTS_DIR.mkdir(parents=True)\n",
    "if not MODELS_DIR.exists():\n",
    "    MODELS_DIR.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03c3ce0",
   "metadata": {},
   "source": [
    "## Define some parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5495aaa",
   "metadata": {},
   "source": [
    "### Number of trainings\n",
    "\n",
    "To achieve more representative outcomes at the conclusion of each exercise, we average the results across multiple training sessions. The `DEFAULT_NUMBER_OF_TRAININGS` variable specifies the number of training sessions conducted before the results are displayed.\n",
    "\n",
    "We recommend setting a lower value (such as 2 or 3) during the development and testing phases of your implementations. Once you have completed your work and are confident in its functionality, you can increase the number of training sessions to minimize the variance in results. Be aware that a higher number of training sessions will extend the execution time, so adjust this setting in accordance with your computer's capabilities.\n",
    "\n",
    "Additionally, you have the option to assign a specific value to the `NUMBER_OF_TRAININGS` variable for each exercise directly within the cells where the training loop is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db979a5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "DEFAULT_NUMBER_OF_TRAININGS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6833664c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Define the video selector widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7a89bd",
   "metadata": {},
   "source": [
    "The `video_selector` function, defined in the next cell, will be used in the following exercises to display different episodes of the trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd46c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_selector(file_path: List[Path]):\n",
    "    return Video(file_path, embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438c38bc",
   "metadata": {},
   "source": [
    "## PyTorch Refresher and Cheat Sheet\n",
    "\n",
    "In this lab, we will be implementing our reinforcement learning algorithms using PyTorch.\n",
    "If you need a refresher, you might find this [PyTorch Cheat Sheet](https://web.archive.org/web/20241215022731/https://pytorch.org/tutorials/beginner/ptcheat.html) helpful. It provides a quick reference for many of the most commonly used PyTorch functions and concepts, and can be a valuable resource as you work through this lab.\n",
    "\n",
    "Autograd, the core concept behind automatic differentiation in PyTorch, is introduced [here](https://pytorch.org/docs/stable/autograd.html).\n",
    "\n",
    "You can also refer to the [official documentation](https://pytorch.org/docs/stable/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ad5a84",
   "metadata": {},
   "source": [
    "## PyTorch setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8579c812",
   "metadata": {},
   "source": [
    "PyTorch can run on both CPUs and GPUs. The following cell will determine the device PyTorch will use. If a GPU is available, PyTorch will use it; otherwise, it will use the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e39c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Set the device to CUDA if available, otherwise use CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b6eaa1",
   "metadata": {},
   "source": [
    "For utilizing a GPU on Google Colab, you also have to activate it following the steps outlined [here](https://colab.research.google.com/notebooks/gpu.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3919d56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available GPUs:\")\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"- Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"- No GPU available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fe8d26",
   "metadata": {},
   "source": [
    "If you have a recent GPU and want to use it, you may need to install a specific version of PyTorch compatible with your CUDA version. For this, you will have to edit the [requirements-lab6.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2026-students/main/requirements-lab6.txt) file and replace the current version of PyTorch with one compatible with your CUDA version. Check the [official PyTorch website](https://pytorch.org/get-started/locally/) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f174e2",
   "metadata": {},
   "source": [
    "Note that a GPU is not very useful in this lab because CartPole is a simple and fast problem to solve, and CUDA spends more time transferring data between the CPU and GPU than processing it directly on the CPU.\n",
    "\n",
    "You can uncomment the next cell to explicitly instruct PyTorch to train neural networks using the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19c6fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f800e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PyTorch will train and test neural networks on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498ebb51",
   "metadata": {},
   "source": [
    "## Setup the Gymnasium environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc171496",
   "metadata": {},
   "source": [
    "As in the previous lab, we will try to solve the CartPole-v1 environment (see: https://gymnasium.farama.org/environments/classic_control/cart_pole/), which offers a continuous state space and a discrete action space.\n",
    "\n",
    "**Reminder**: the CartPole task consists of keeping a pole upright by moving the cart to which it is attached via a joint.\n",
    "No friction is considered.\n",
    "The task is considered solved if the pole stays upright (within 15 degrees) for 500 steps on average over 100 episodes, while keeping the cart position within reasonable bounds.\n",
    "The state is given by $\\{x,\\dot{x},\\phi,\\dot{\\phi}\\}$ where $x$ is the position of the cart and $\\phi$ is the angle between the pole and the vertical.\n",
    "There are only two possible actions: $a \\in \\{0, 1\\}$ where $a = 0$ means \"push the cart to the LEFT\" and $a = 1$ means \"push the cart to the RIGHT\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7686d9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "cartpole_observation_space = cast(gym.spaces.Box, env.observation_space)\n",
    "cartpole_action_space = cast(gym.spaces.Discrete, env.action_space)\n",
    "\n",
    "cartpole_observation_dim:int = cartpole_observation_space.shape[0]    # Number of *dimensions* in the observation space (i.e. number of *elements* in the observation vector)\n",
    "cartpole_num_actions:int = cast(int, cartpole_action_space.n.item())  # Number of possible actions\n",
    "\n",
    "print(f\"State space size is: {cartpole_observation_space}\")\n",
    "print(f\"Action space size is: {cartpole_action_space}\")\n",
    "print(\"Actions are: {\" + \", \".join([str(a) for a in range(cartpole_num_actions)]) + \"}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9fd52a",
   "metadata": {},
   "source": [
    "## Notation Table\n",
    "\n",
    "The following table summarizes the main symbols and notations used throughout this lab.\n",
    "\n",
    "| Symbol                                                                  | Meaning                                                                                |\n",
    "|-------------------------------------------------------------------------|----------------------------------------------------------------------------------------|\n",
    "| $s, s_t$                                                                | State (or observation) / state (or observation) at time step $t$                       |\n",
    "| $a, a_t$                                                                | Action / action at time step $t$                                                       |\n",
    "| $r_t$ or $r(s_t, a_t)$                                                  | Reward received at time step $t$                                                       |\n",
    "| $T$                                                                     | Horizon (maximum number of steps per episode)                                          |\n",
    "| $\\pi_{\\boldsymbol{\\theta}}$                                             | Parameterized stochastic policy with parameters $\\boldsymbol{\\theta}$                  |\n",
    "| $\\pi_{\\boldsymbol{\\theta}}(a \\mid s)$                                   | Probability of taking action $a$ in state $s$ under policy $\\pi_{\\boldsymbol{\\theta}}$ |\n",
    "| $\\boldsymbol{\\theta} \\in \\mathbb{R}^d$                                  | Policy parameters (neural network weights)                                             |\n",
    "| $\\alpha$                                                                | Learning rate                                                                          |\n",
    "| $J(\\boldsymbol{\\theta})$                                                | Optimization criterion (expected total return)                                         |\n",
    "| $V^{\\pi_{\\boldsymbol{\\theta}}}(s)$                                      | State-value function under policy $\\pi_{\\boldsymbol{\\theta}}$                          |\n",
    "| $Q^{\\pi_{\\boldsymbol{\\theta}}}(s, a)$                                   | Action-value function under policy $\\pi_{\\boldsymbol{\\theta}}$                         |\n",
    "| $G_t = \\sum_{k=t}^{T-1} r_k$                                            | Monte Carlo return from time step $t$                                                  |\n",
    "| $\\nabla_{\\boldsymbol{\\theta}} \\log \\pi_{\\boldsymbol{\\theta}}(a \\mid s)$ | Score function (gradient of the log-policy)                                            |\n",
    "| $\\tau$                                                                  | Trajectory $\\{ s_0, a_0, r_0, s_1, a_1, \\dots, r_{T-1}, s_T \\}$                        |\n",
    "| $\\mathbb{E}_{\\pi_{\\boldsymbol{\\theta}}}[\\cdot]$                         | Expectation under policy $\\pi_{\\boldsymbol{\\theta}}$                                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03883df",
   "metadata": {},
   "source": [
    "## Part 1: Monte Carlo Policy Gradient (REINFORCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd38a32a",
   "metadata": {},
   "source": [
    "### The Policy Gradient theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0b06a3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "We will solve the CartPole environment using a policy gradient method which directly searches, within a family of parameterized policies $\\pi_\\theta$, for an optimal policy.\n",
    "\n",
    "This method performs gradient ascent in the policy space so that the total return is maximized.\n",
    "We will restrict our work to episodic tasks, *i.e.* tasks that have a starting state and last for a finite and fixed number of steps $T$, called the horizon.\n",
    "\n",
    "More formally, we define an optimization criterion that we want to maximize:\n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}_{\\pi_\\theta}\\left[\\sum_{t=0}^{T-1} r(s_t,a_t)\\right],$$\n",
    "\n",
    "where $\\mathbb{E}_{\\pi_\\theta}$ means $a \\sim \\pi_\\theta(\\cdot|s)$ and $T$ is the horizon of the episode.\n",
    "In other words, we want to maximize the value of the starting state: $V^{\\pi_\\theta}(s)$.\n",
    "The policy gradient theorem tells us that:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\nabla_\\theta V^{\\pi_\\theta}(s) = \\mathbb{E}_{\\pi_\\theta} \\Bigl[\\nabla_\\theta \\log \\pi_\\theta (a|s) ~ Q^{\\pi_\\theta}(s,a) \\Bigr],\n",
    "$$\n",
    "\n",
    "where the $Q$-function is defined as:\n",
    "\n",
    "$$Q^{\\pi_\\theta}(s,a) = \\mathbb{E}_{\\pi_\\theta} \\left[\\sum_{t=0}^{T-1} r(s_t,a_t)|s=s_0, a=a_0\\right].$$\n",
    "\n",
    "The policy gradient theorem is particularly effective because it allows gradient computation without needing to model the system's dynamics, as long as the $Q$-function (or an estimate of it) is available. By simply applying the policy and observing one-step transitions, sufficient information can be gathered. Using stochastic gradient ascent and substituting $Q^{\\pi_\\theta}(s_t,a_t)$ with a Monte Carlo estimate $G_t = \\sum_{t'=t}^{T-1} r(s_{t'},a_{t'})$ for a single trajectory, we obtain the REINFORCE algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff7504c",
   "metadata": {},
   "source": [
    "The REINFORCE algorithm, introduced by Williams in 1992, is a Monte Carlo policy gradient method. It updates the policy in the direction that maximizes rewards, using full-episode returns as an unbiased estimate of the gradient. Each step involves generating an episode using the current policy, computing the gradient estimate, and updating the policy parameters. This algorithm is simple yet powerful, and it's particularly effective in environments where the policy gradient is noisy or the dynamics are complex.\n",
    "\n",
    "For further reading and a deeper understanding, refer to Williams' seminal paper (https://link.springer.com/article/10.1007/BF00992696) and the comprehensive text on reinforcement learning by Richard S. Sutton and Andrew G. Barto: \"Reinforcement Learning: An Introduction\", chap.13 (http://incompleteideas.net/book/RLbook2020.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7341645",
   "metadata": {},
   "source": [
    "Here is the REINFORCE algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbb7e12",
   "metadata": {},
   "source": [
    "### Monte Carlo policy gradient (REINFORCE)\n",
    "\n",
    "<b>REQUIRE</b> <br>\n",
    "$\\quad$ A differentiable policy $\\pi_{\\boldsymbol{\\theta}}$ <br>\n",
    "$\\quad$ A learning rate $\\alpha \\in \\mathbb{R}^+$ <br>\n",
    "<b>INITIALIZATION</b> <br>\n",
    "$\\quad$ Initialize parameters $\\boldsymbol{\\theta} \\in \\mathbb{R}^d$ <br>\n",
    "<br>\n",
    "<b>FOR EACH</b> episode <br>\n",
    "$\\quad$ Generate full trace $\\tau = \\{ \\boldsymbol{s}_0, \\boldsymbol{a}_0, r_0, \\boldsymbol{s}_1, \\boldsymbol{a}_1, \\dots, r_{T-1}, \\boldsymbol{s}_T \\}$ following $\\pi_{\\boldsymbol{\\theta}}$ <br>\n",
    "$\\quad$ <b>FOR</b> $~ t=0,\\dots,T-1$ <br>\n",
    "$\\quad\\quad$ $G \\leftarrow \\sum_{k=t}^{T-1} r_k$ <br>\n",
    "$\\quad\\quad$ $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} + \\alpha ~ \\underbrace{G ~ \\nabla_{\\boldsymbol{\\theta}} \\ln \\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{a}_t|\\boldsymbol{s}_t)}_{\\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta})}$ <br>\n",
    "<br>\n",
    "<b>RETURN</b> $\\boldsymbol{\\theta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf09948f",
   "metadata": {},
   "source": [
    "### Exercise 1: REINFORCE for discrete action spaces (CartPole)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e843aa4",
   "metadata": {},
   "source": [
    "#### Policy Implementation\n",
    "\n",
    "We will implement a stochastic policy to control the cart using a simple one-layer neural network. Given the simplicity of the problem, a single layer will suffice. We will not incorporate a bias term in this layer.\n",
    "\n",
    "This neural network will output the probabilities of each possible action (in this case, there are only two actions: \"push left\" or \"push right\") given the input vector $s$ (the 4-dimensional state vector)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6e2191",
   "metadata": {},
   "source": [
    "**Task 1.1**: Implement the `PolicyNetwork` defined as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e03f7ea",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "The network takes an input tensor representing the state of the environment and outputs a tensor of action probabilities.\n",
    "The network has the following components:\n",
    "\n",
    "- `layer1`: This is a linear (fully connected) layer that takes `n_observations` as input and outputs `n_actions`. It does not include a bias term due to the symmetry of the CartPole problem around 0.\n",
    "\n",
    "- `forward` method: This method defines the forward pass of the network. It takes a state tensor as input and returns a tensor of action probabilities. It first applies the linear layer to the input state tensor to get the logits (the raw, unnormalized scores for each action), and then applies the [softmax function](https://docs.pytorch.org/docs/stable/generated/torch.softmax.html#torch-softmax) to the logits to get the action probabilities. The [softmax function](https://docs.pytorch.org/docs/stable/generated/torch.softmax.html#torch-softmax) ensures that the action probabilities are positive and sum to 1, so they can be interpreted as probabilities.\n",
    "\n",
    "This network is quite simple and may not perform well on complex tasks with large state or action spaces. However, it can be a good starting point for simple reinforcement learning tasks, and can be easily extended with more layers or different types of layers (such as convolutional layers for image inputs) to handle more complex tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166da0ee",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network used as a policy for the REINFORCE algorithm.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    layer1 : torch.nn.Linear\n",
    "        A fully connected layer.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(state: torch.Tensor) -> torch.Tensor\n",
    "        Define the forward pass of the PolicyNetwork.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_observations: int, n_actions: int):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of PolicyNetwork.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_observations : int\n",
    "            The size of the observation space.\n",
    "        n_actions : int\n",
    "            The size of the action space.\n",
    "        \"\"\"\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "\n",
    "        self.layer1 = ... # TODO\n",
    "\n",
    "\n",
    "    def forward(self, state_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate the probability of each action for the given state.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state_tensor : torch.Tensor\n",
    "            The input tensor (state).\n",
    "            The shape of the tensor should be (N, dim),\n",
    "            where N is the number of states vectors in the batch\n",
    "            and dim is the dimension of state vectors.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The output tensor (the probability of each action for the given state).\n",
    "        \"\"\"\n",
    "\n",
    "        logits = ... # TODO\n",
    "        out = ...    # TODO\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8dc122",
   "metadata": {},
   "source": [
    "#### Implement the `sample_discrete_action` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b30c770",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "**Task 1.2**: Complete the `sample_discrete_action` function. This function is used to sample a discrete action based on a given state and a policy network. It first converts the state into a tensor and passes it through the policy network to get the parameters of the action probability distribution. Then, it creates a [categorical distribution](https://docs.pytorch.org/docs/stable/distributions.html#torch.distributions.categorical.Categorical) from these parameters and [samples an action](https://docs.pytorch.org/docs/stable/distributions.html#torch.distributions.categorical.Categorical.sample) from this distribution. It also [calculates the log probability](https://docs.pytorch.org/docs/stable/distributions.html#torch.distributions.categorical.Categorical.log_prob) of the sampled action according to the distribution. The function returns the sampled action and its log probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6362bf9a",
   "metadata": {},
   "source": [
    "**Practical tips**: To complete this task, you need to be familiar with the following PyTorch concepts:\n",
    "\n",
    "- [torch.distributions.Categorical](https://docs.pytorch.org/docs/stable/distributions.html#torch.distributions.categorical.Categorical): the categorical distribution.\n",
    "- [torch.distributions.Categorical.sample()](https://docs.pytorch.org/docs/stable/distributions.html#torch.distributions.categorical.Categorical.sample): sample from the categorical distribution.\n",
    "- [torch.distributions.Categorical.log_prob()](https://docs.pytorch.org/docs/stable/distributions.html#torch.distributions.categorical.Categorical.log_prob): calculate the log probability of an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9898fd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_discrete_action(\n",
    "    policy_nn: PolicyNetwork, state: np.ndarray\n",
    ") -> Tuple[int, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Sample a discrete action based on the given state and policy network.\n",
    "\n",
    "    This function takes a state and a policy network, and returns a sampled action and its log probability.\n",
    "    The action is sampled from a categorical distribution defined by the output of the policy network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    policy_nn : PolicyNetwork\n",
    "        The policy network that defines the probability distribution of the actions.\n",
    "    state : np.ndarray\n",
    "        The state based on which an action needs to be sampled.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[int, torch.Tensor]\n",
    "        The sampled action and its log probability.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the state into a tensor, specify its data type as float32, and send it to the device (CPU or GPU).\n",
    "    # The unsqueeze(0) function is used to add an extra dimension to the tensor to match the input shape required by the policy network.\n",
    "    state_tensor = ... # TODO\n",
    "\n",
    "    # Pass the state tensor through the policy network to get the parameters of the action probability distribution.\n",
    "    actions_probability_distribution_params = ... # TODO\n",
    "\n",
    "    # Create the categorical distribution used to sample an action from the parameters obtained from the policy network.\n",
    "    # See https://pytorch.org/docs/stable/distributions.html#categorical\n",
    "    actions_probability_distribution = ... # TODO\n",
    "\n",
    "    # Sample an action from the categorical distribution.\n",
    "    sampled_action_tensor = ... # TODO\n",
    "\n",
    "    # Convert the tensor containing the sampled action into a Python integer.\n",
    "    sampled_action = ... # TODO\n",
    "\n",
    "    # Calculate the log probability of the sampled action according to the categorical distribution.\n",
    "    sampled_action_log_probability = ... # TODO\n",
    "\n",
    "    # Return the sampled action and its log probability.\n",
    "    return sampled_action, sampled_action_log_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1ead22",
   "metadata": {},
   "source": [
    "**Task 1.3**: Test the `sample_discrete_action` function on a random state using an untrained policy network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5be2b78",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "policy_nn = ... # TODO\n",
    "\n",
    "state = ... # TODO\n",
    "theta = ... # TODO\n",
    "action, action_log_probability = ... # TODO\n",
    "\n",
    "print(\"state:\", state)\n",
    "print(\"theta:\", theta)\n",
    "print(\"sampled action:\", action)\n",
    "print(\"log probability of the sampled action:\", action_log_probability)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0beb60",
   "metadata": {},
   "source": [
    "#### Implement the `sample_one_episode` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9517a9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Remember that in the REINFORCE algorithm, we need to generate a complete trajectory, denoted as $\\tau = \\{ \\boldsymbol{s}_0, \\boldsymbol{a}_0, r_0, \\boldsymbol{s}_1, \\boldsymbol{a}_1, \\dots, r_{T-1}, \\boldsymbol{s}_T \\}$. This trajectory includes the states, actions, and rewards at each time step, as outlined in the REINFORCE algorithm.\n",
    "\n",
    "**Task 1.4**: Your task is to implement the `sample_one_episode` function. This function should play one episode using the given policy $\\pi_\\theta$ and return its rollouts. The function should adhere to a fixed horizon $T$, which represents the maximum number of steps in the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4270d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_one_episode(\n",
    "    env: gym.Env, policy_nn: PolicyNetwork, max_episode_duration: int\n",
    ") -> Tuple[List[np.ndarray], List[int], List[float], List[torch.Tensor]]:\n",
    "    \"\"\"Execute one episode in `env` using the policy defined by `policy_nn`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to play in.\n",
    "    policy_nn : PolicyNetwork\n",
    "        The policy neural network.\n",
    "    max_episode_duration : int\n",
    "        The maximum duration of the episode.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[List[np.ndarray], List[int], List[float], List[torch.Tensor]]\n",
    "        The states, actions, rewards, and log-probability of the action for each time step in the episode.\n",
    "    \"\"\"\n",
    "    state_t, info = env.reset()\n",
    "\n",
    "    episode_states = []\n",
    "    episode_actions = []\n",
    "    episode_log_prob_actions = []\n",
    "    episode_rewards = []\n",
    "    episode_states.append(state_t)\n",
    "\n",
    "    for t in range(max_episode_duration):\n",
    "\n",
    "        # Sample a discrete action and its log probability from the policy network based on the current state\n",
    "        action_t, log_prob_action_t = ... # TODO\n",
    "\n",
    "        # Execute the sampled action in the environment, which returns the new state, reward, and whether the episode has terminated or been truncated\n",
    "        state_t, reward_t, terminated, truncated, info = env.step(action_t)\n",
    "\n",
    "        # Check if the episode is done, either due to termination (reaching a terminal state) or truncation (reaching a maximum number of steps)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Append the new state, action, action log probability and reward to their respective lists\n",
    "\n",
    "        episode_states.append(state_t)\n",
    "        episode_actions.append(action_t)\n",
    "        episode_log_prob_actions.append(log_prob_action_t)\n",
    "        episode_rewards.append(float(reward_t))\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return episode_states, episode_actions, episode_rewards, episode_log_prob_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0993049",
   "metadata": {},
   "source": [
    "**Task 1.5:** Test this function on the untrained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bd23ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PREFIX_EX1_REINFORCE_UNTRAINED = \"lab6_ex1_reinforce_untrained\"\n",
    "\n",
    "NUM_EPISODES = 3\n",
    "\n",
    "file_path_list = [\n",
    "    FIGS_DIR / f\"{VIDEO_PREFIX_EX1_REINFORCE_UNTRAINED}-episode-{episode_index}.mp4\"\n",
    "    for episode_index in range(NUM_EPISODES)\n",
    "]\n",
    "\n",
    "for file_path in file_path_list:\n",
    "    file_path.unlink(missing_ok=True)\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(\n",
    "    env,\n",
    "    video_folder=str(FIGS_DIR),\n",
    "    name_prefix=VIDEO_PREFIX_EX1_REINFORCE_UNTRAINED,\n",
    "    episode_trigger=lambda x: True,\n",
    ")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=NUM_EPISODES)\n",
    "\n",
    "for episode_index in range(NUM_EPISODES):\n",
    "    policy_nn = ... # TODO\n",
    "    episode_states, episode_actions, episode_rewards, episode_log_prob_actions = ... # TODO\n",
    "\n",
    "print(f\"Episode time taken: {env.time_queue}\")\n",
    "print(f\"Episode total rewards: {env.return_queue}\")\n",
    "print(f\"Episode lengths: {env.length_queue}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"\\nSelect the episode to play here ðŸ‘‡\\n\")\n",
    "\n",
    "interact(video_selector, file_path=file_path_list);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcc2e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671e8aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15319d26",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b6c92b",
   "metadata": {},
   "source": [
    "#### Implement a test function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81123a49",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "**Task 1.6**: Implement the `avg_return_on_multiple_episodes` function that tests the given policy $\\pi_\\theta$ over `num_test_episode` episodes (with a fixed horizon $T$) and returns the average return across these episodes.\n",
    "\n",
    "The function `avg_return_on_multiple_episodes` is designed to play multiple episodes of a given environment using a specified policy neural network and to compute the average return. It takes as input the environment to play in, the policy neural network to use, the number of episodes to play, and the maximum duration of an episode.\n",
    "In each episode, it uses the `sample_one_episode` function to play the episode and collect the rewards. The function then returns the average of these cumulative returns.\n",
    "\n",
    "`avg_return_on_multiple_episodes` will be used for evaluating the performance of a policy over multiple episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbabfd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_return_on_multiple_episodes(\n",
    "    env: gym.Env,\n",
    "    policy_nn: PolicyNetwork,\n",
    "    num_test_episode: int,\n",
    "    max_episode_duration: int,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Play multiple episodes of the environment and calculate the average return.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to play in.\n",
    "    policy_nn : PolicyNetwork\n",
    "        The policy neural network.\n",
    "    num_test_episode : int\n",
    "        The number of episodes to play.\n",
    "    max_episode_duration : int\n",
    "        The maximum duration of an episode.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The average return.\n",
    "    \"\"\"\n",
    "\n",
    "    ... # TODO\n",
    "\n",
    "    return average_return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea87098",
   "metadata": {},
   "source": [
    "**Task 1.7:** Test this function on the untrained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d612fc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "policy_nn = ... # TODO\n",
    "average_return = ... # TODO\n",
    "\n",
    "print(average_return)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f681eca4",
   "metadata": {},
   "source": [
    "#### Implement the train function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198867c2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "**Task 1.8**: Implement the `train_reinforce_discrete` function, used to train a policy network using the REINFORCE algorithm in the given environment. This function takes as input the environment, the number of training episodes, the number of tests to perform per episode, the maximum duration of an episode, and the learning rate for the optimizer.\n",
    "\n",
    "The function first initializes a policy network and an Adam optimizer. Then, for each training episode, it generates an episode using the current policy and calculates the return at each time step. It uses this return and the log probability of the action taken at that time step to compute the loss, which is the negative of the product of the return and the log probability. This loss is used to update the policy network parameters using gradient ascent.\n",
    "\n",
    "After each training episode, the function tests the current policy by playing a number of test episodes and calculating the average return. This average return is added to a list for monitoring purposes.\n",
    "\n",
    "The function returns the trained policy network and the list of average returns for each episode. This function encapsulates the main loop of the REINFORCE algorithm, including the policy update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db22f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reinforce_discrete(\n",
    "    env: gym.Env,\n",
    "    num_train_episodes: int,\n",
    "    num_test_per_episode: int,\n",
    "    max_episode_duration: int,\n",
    "    learning_rate: float,\n",
    ") -> Tuple[PolicyNetwork, List[float]]:\n",
    "    \"\"\"\n",
    "    Train a policy using the REINFORCE algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to train in.\n",
    "    num_train_episodes : int\n",
    "        The number of training episodes.\n",
    "    num_test_per_episode : int\n",
    "        The number of tests to perform per episode.\n",
    "    max_episode_duration : int\n",
    "        The maximum length of an episode.\n",
    "    learning_rate : float\n",
    "        The learning rate for the Adam optimizer.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[PolicyNetwork, List[float]]\n",
    "        The final trained policy and the average evaluation returns computed after each training episode.\n",
    "    \"\"\"\n",
    "    episode_avg_return_list = []\n",
    "\n",
    "    policy_nn = PolicyNetwork(cartpole_observation_dim, cartpole_num_actions).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(policy_nn.parameters(), lr=learning_rate)\n",
    "\n",
    "    for episode_index in tqdm(range(num_train_episodes)):\n",
    "\n",
    "        # Generate an episode following the current policy\n",
    "        _, _, episode_reward_list, episode_log_prob_action_list = ... # TODO\n",
    "\n",
    "        # Iterate over the episode\n",
    "        for t in range(len(episode_reward_list)):\n",
    "            # Calculate the return at time t\n",
    "            future_return = ... # TODO\n",
    "\n",
    "            # Convert the future_return to a PyTorch tensor\n",
    "            returns_tensor = ... # TODO\n",
    "\n",
    "            # Get the log probability of the action taken at time t\n",
    "            log_prob_actions_tensor = ... # TODO\n",
    "\n",
    "            # Gradient descent on the negated objective (equivalent to gradient ascent on J(Î¸))\n",
    "            loss = ... # TODO\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Test the current policy\n",
    "        test_avg_return = avg_return_on_multiple_episodes(\n",
    "            env=env,\n",
    "            policy_nn=policy_nn,\n",
    "            num_test_episode=num_test_per_episode,\n",
    "            max_episode_duration=max_episode_duration,\n",
    "        )\n",
    "\n",
    "        # Monitoring\n",
    "        episode_avg_return_list.append(test_avg_return)\n",
    "\n",
    "    return policy_nn, episode_avg_return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4bf80e",
   "metadata": {},
   "source": [
    "#### Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e5cafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "NUMBER_OF_TRAININGS = DEFAULT_NUMBER_OF_TRAININGS  # Change the default (global) value here if you want a specific number of trainings for this exercise\n",
    "reinforce_trains_result_list: List[List[Union[int, float]]] = [[], [], []]\n",
    "\n",
    "for train_index in range(NUMBER_OF_TRAININGS):\n",
    "    # Train the agent\n",
    "    reinforce_policy_nn, episode_reward_list = train_reinforce_discrete(\n",
    "        env=env,\n",
    "        num_train_episodes=150,\n",
    "        num_test_per_episode=5,\n",
    "        max_episode_duration=500,\n",
    "        learning_rate=0.005,\n",
    "    )\n",
    "\n",
    "    reinforce_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
    "    reinforce_trains_result_list[1].extend(episode_reward_list)\n",
    "    reinforce_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
    "\n",
    "reinforce_trains_result_df = pd.DataFrame(\n",
    "    np.array(reinforce_trains_result_list).T,\n",
    "    columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"],\n",
    ")\n",
    "reinforce_trains_result_df[\"agent\"] = \"REINFORCE\"\n",
    "\n",
    "# Save the action-value estimation function of the last train\n",
    "\n",
    "torch.save(reinforce_policy_nn, MODELS_DIR / \"lab6_reinforce_policy_network.pth\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58def64c",
   "metadata": {},
   "source": [
    "#### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7111ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    x=\"num_episodes\",\n",
    "    y=\"mean_final_episode_reward\",\n",
    "    kind=\"line\",\n",
    "    hue=\"agent\",\n",
    "    estimator=None,\n",
    "    units=\"training_index\",\n",
    "    data=reinforce_trains_result_df,\n",
    "    height=7,\n",
    "    aspect=2,\n",
    "    alpha=0.5,\n",
    ")\n",
    "plt.savefig(PLOTS_DIR / \"lab6_reinforce_cartpole_trains_result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0763eab",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "all_trains_result_df = pd.concat(\n",
    "    [\n",
    "        reinforce_trains_result_df,\n",
    "    ]\n",
    ")\n",
    "g = sns.relplot(\n",
    "    x=\"num_episodes\",\n",
    "    y=\"mean_final_episode_reward\",\n",
    "    kind=\"line\",\n",
    "    hue=\"agent\",\n",
    "    data=all_trains_result_df,\n",
    "    height=7,\n",
    "    aspect=2,\n",
    ")\n",
    "plt.savefig(PLOTS_DIR / \"lab6_reinforce_cartpole_trains_result_agg.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50601fb2",
   "metadata": {},
   "source": [
    "#### Test final policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdf4634",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PREFIX_EX1_REINFORCE_TRAINED = \"lab6_ex1_reinforce_trained\"\n",
    "\n",
    "NUM_EPISODES = 3\n",
    "\n",
    "file_path_list = [\n",
    "    FIGS_DIR / f\"{VIDEO_PREFIX_EX1_REINFORCE_TRAINED}-episode-{episode_index}.mp4\"\n",
    "    for episode_index in range(NUM_EPISODES)\n",
    "]\n",
    "\n",
    "for file_path in file_path_list:\n",
    "    file_path.unlink(missing_ok=True)\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(\n",
    "    env,\n",
    "    video_folder=str(FIGS_DIR),\n",
    "    name_prefix=VIDEO_PREFIX_EX1_REINFORCE_TRAINED,\n",
    "    episode_trigger=lambda x: True,\n",
    ")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=NUM_EPISODES)\n",
    "\n",
    "for episode_index in range(NUM_EPISODES):\n",
    "    episode_states, episode_actions, episode_rewards, episode_log_prob_actions = ... # TODO\n",
    "\n",
    "print(f\"Episode time taken: {env.time_queue}\")\n",
    "print(f\"Episode total rewards: {env.return_queue}\")\n",
    "print(f\"Episode lengths: {env.length_queue}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"\\nSelect the episode to play here ðŸ‘‡\\n\")\n",
    "\n",
    "interact(video_selector, file_path=file_path_list);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd689539",
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce_trains_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b8dbeb",
   "metadata": {},
   "source": [
    "#### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340029c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_ex1 = reinforce_trains_result_df[[\"num_episodes\", \"mean_final_episode_reward\"]].groupby(\"num_episodes\").mean().max()\n",
    "score_ex1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d172069",
   "metadata": {},
   "source": [
    "### Exercise 2: REINFORCE with Baseline\n",
    "\n",
    "In the basic REINFORCE algorithm, the policy parameters are updated in proportion to the product of the gradient of the policy log-probability and the cumulative reward (return) from a state-action pair. However, this approach can lead to high variance in policy updates, making learning slower.\n",
    "\n",
    "In this exercise, a *baseline* is introduced to reduce this variance. It is a value subtracted from the cumulative reward when calculating the policy gradient. The key property of the baseline is that it does not affect the expected value of the gradient estimate, which means it doesn't bias the learning process but reduces the variance of the updates.\n",
    "\n",
    "The baseline can be thought of as a reference point or an \"average\" expectation of reward. By comparing the actual rewards to this baseline, we can determine whether the outcomes of certain actions are better or worse than this \"average\" performance.\n",
    "\n",
    "A common choice for the baseline is the value function of the current policy, $\\hat{V}_{\\boldsymbol{\\omega}}$. By using the value function as a baseline, the algorithm adjusts the policy towards actions that perform better than the average.\n",
    "\n",
    "To incorporate the baseline into REINFORCE, you modify the update rule. Instead of using the total return $G$â€‹ directly, you subtract the baseline value $\\hat{V}_{\\boldsymbol{\\omega}}$ from $G$â€‹ in the policy gradient estimate.\n",
    "\n",
    "By centering the rewards around a baseline, the variance of the policy gradient estimates is reduced. This leads to more stable and efficient learning, as the updates are less noisy and more focused on improving relative to the average performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fa04db",
   "metadata": {},
   "source": [
    "#### REINFORCE with Baseline\n",
    "\n",
    "<b>REQUIRE</b> <br>\n",
    " $\\quad$ A differentiable policy $\\pi_{\\boldsymbol{\\theta}}$ <br>\n",
    " $\\quad$ A differentiable baseline function $\\hat{V}_{\\boldsymbol{\\omega}}(\\boldsymbol{s})$ <br>\n",
    " $\\quad$ A learning rate $\\alpha_1 \\in \\mathbb{R}^+$ for the policy <br>\n",
    " $\\quad$ A learning rate $\\alpha_2 \\in \\mathbb{R}^+$ for the baseline <br>\n",
    "<b>INITIALIZATION</b> <br>\n",
    " $\\quad$ Initialize parameters $\\boldsymbol{\\theta} \\in \\mathbb{R}^d$ <br>\n",
    " $\\quad$ Initialize parameters $\\boldsymbol{\\omega} \\in \\mathbb{R}^d$ <br>\n",
    "<br>\n",
    "<b>FOR EACH</b> episode <br>\n",
    " $\\quad$ Generate full trace $\\tau = \\{ \\boldsymbol{s}_0, \\boldsymbol{a}_0, r_0, \\boldsymbol{s}_1, \\boldsymbol{a}_1, \\dots, r_{T-1}, \\boldsymbol{s}_T \\}$ following $\\pi_{\\boldsymbol{\\theta}}$ <br>\n",
    " $\\quad$ <b>FOR</b> $~ t=0,\\dots,T-1$ <br>\n",
    "  $\\quad\\quad$ $G \\leftarrow \\sum_{k=t}^{T-1} r_k$ <br>\n",
    "  $\\quad\\quad$ $\\delta_t \\leftarrow G - \\hat{V}_{\\boldsymbol{\\omega}}(\\boldsymbol{s}_t)$ <br>\n",
    "  $\\quad\\quad$ $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} + \\alpha_1 ~ \\delta_t ~ \\nabla_{\\boldsymbol{\\theta}} \\ln \\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{a}_t|\\boldsymbol{s}_t)$ <br>\n",
    "  $\\quad\\quad$ $\\boldsymbol{\\omega} \\leftarrow \\boldsymbol{\\omega} + \\alpha_2 ~ \\delta_t \\nabla_{\\boldsymbol{\\omega}}\\hat{V}_{\\boldsymbol{\\omega}}(\\boldsymbol{s}_t) $ <br>\n",
    "<br>\n",
    "<b>RETURN</b> $\\boldsymbol{\\theta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe3d375",
   "metadata": {},
   "source": [
    "**Task 2.1**: Implement the `ValueNetwork` ($\\hat{V}_{\\boldsymbol{\\omega}}$ in the algorithm) defined as follow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa78fa9c",
   "metadata": {},
   "source": [
    "`ValueNetwork` is a simple linear model. It takes an input tensor representing the state of the environment and outputs a tensor representing the estimated value of that state. The input tensor's shape should be (N, dim), where N is the number of state vectors in the batch and dim is the dimension of the state vectors.\n",
    "\n",
    "The network has the following components:\n",
    "- `linear_layer`: This is a linear (fully connected) layer that takes `n_observations` as input and outputs a single value.\n",
    "- `forward` method: This method defines the forward pass of the network. It takes a state tensor as input and returns a tensor representing the estimated value of the state. It applies the linear layer to the input to get the final output.\n",
    "\n",
    "This network is quite simple and may not perform well on complex tasks with large state spaces. However, it can be a good starting point for simple reinforcement learning tasks, and can be easily extended with more layers or different types of layers (such as convolutional layers for image inputs) to handle more complex tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6893581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A linear model that estimates the value of a state.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_observations : int\n",
    "        The number of observations in the state.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    linear_layer : torch.nn.Linear\n",
    "        A fully connected layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_observations: int):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "\n",
    "        self.linear_layer = ... # TODO\n",
    "\n",
    "\n",
    "    def forward(self, observation_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation_tensor : torch.Tensor\n",
    "            The input tensor representing the observation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The output tensor representing the value of the observation.\n",
    "        \"\"\"\n",
    "\n",
    "        x = ... # TODO\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d6b7a8",
   "metadata": {},
   "source": [
    "#### Implement the train function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b9a7fe",
   "metadata": {},
   "source": [
    "**Task 2.2**: Implement the `train_reinforce_baseline_discrete` function, used to train a policy network and a value network using the REINFORCE with baseline algorithm in a given environment.\n",
    "\n",
    "The function first initializes a policy network and a value network, along with their respective Adam optimizers. Then, for each training episode, it generates an episode using the current policy and calculates the return at each time step. It uses this return, the log probability of the action taken at that time step, and the estimated value of the state to compute the policy and value losses. These losses are used to update the policy and value network parameters using gradient ascent. The value loss is typically defined as the squared difference between the estimated return and the actual return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95850637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reinforce_baseline_discrete(\n",
    "    env: gym.Env,\n",
    "    num_train_episodes: int,\n",
    "    num_test_per_episode: int,\n",
    "    max_episode_duration: int,\n",
    "    policy_learning_rate: float,\n",
    "    value_learning_rate: float,\n",
    ") -> Tuple[PolicyNetwork, List[float]]:\n",
    "    \"\"\"\n",
    "    Train a policy using the REINFORCE with baseline algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to train in.\n",
    "    num_train_episodes : int\n",
    "        The number of training episodes.\n",
    "    num_test_per_episode : int\n",
    "        The number of tests to perform per episode.\n",
    "    max_episode_duration : int\n",
    "        The maximum length of an episode.\n",
    "    policy_learning_rate : float\n",
    "        The policy learning rate.\n",
    "    value_learning_rate : float\n",
    "        The value learning rate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[PolicyNetwork, List[float]]\n",
    "        The final trained policy and the average returns for each episode.\n",
    "    \"\"\"\n",
    "    episode_avg_return_list = []\n",
    "\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n.item()\n",
    "\n",
    "    policy_nn = PolicyNetwork(state_size, action_size).to(DEVICE)\n",
    "    policy_optimizer = torch.optim.Adam(policy_nn.parameters(), lr=policy_learning_rate)\n",
    "\n",
    "    value_nn = ValueNetwork(state_size).to(DEVICE)\n",
    "    value_optimizer = torch.optim.Adam(value_nn.parameters(), lr=value_learning_rate)\n",
    "\n",
    "    value_loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    for episode_index in tqdm(range(num_train_episodes)):\n",
    "        # Generate an episode following the current policy\n",
    "        episode_state_list, _, episode_reward_list, episode_log_prob_action_list = (\n",
    "            sample_one_episode(\n",
    "                env=env, policy_nn=policy_nn, max_episode_duration=max_episode_duration\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Iterate over the episode\n",
    "        for t in range(len(episode_reward_list)):\n",
    "\n",
    "            # Calculate the return G_t at time step t\n",
    "            G_t = ... # TODO\n",
    "\n",
    "            # Convert G_t to a PyTorch tensor\n",
    "            returns_tensor = ... # TODO\n",
    "\n",
    "            # Get the log probability of the action taken at time t\n",
    "            log_prob_actions_tensor = ... # TODO\n",
    "\n",
    "            # Convert the state s_t to a PyTorch tensor\n",
    "            state_tensor = ... # TODO\n",
    "\n",
    "            # Compute the baseline value V(s_t)\n",
    "            state_value = ... # TODO\n",
    "\n",
    "            # Compute the advantage: delta_t = G_t - V(s_t)\n",
    "            advantage = ... # TODO\n",
    "\n",
    "            # Compute the policy loss (negative because we maximize J(Î¸))\n",
    "            policy_loss = ... # TODO\n",
    "\n",
    "            # Update the policy network\n",
    "            policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            policy_optimizer.step()\n",
    "\n",
    "            # Compute the value loss (MSE between predicted value and actual return)\n",
    "            value_loss = ... # TODO\n",
    "\n",
    "            # Update the value network\n",
    "            value_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            value_optimizer.step()\n",
    "\n",
    "        # Test the current policy\n",
    "        test_avg_return = avg_return_on_multiple_episodes(\n",
    "            env=env,\n",
    "            policy_nn=policy_nn,\n",
    "            num_test_episode=num_test_per_episode,\n",
    "            max_episode_duration=max_episode_duration\n",
    "        )\n",
    "\n",
    "        # Monitoring\n",
    "        episode_avg_return_list.append(test_avg_return)\n",
    "\n",
    "    return policy_nn, episode_avg_return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e038b3c",
   "metadata": {},
   "source": [
    "#### Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a19f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "NUMBER_OF_TRAININGS = DEFAULT_NUMBER_OF_TRAININGS  # Change the default (global) value here if you want a specific number of trainings for this exercise\n",
    "reinforce_baseline_trains_result_list = [[], [], []]\n",
    "\n",
    "for train_index in range(NUMBER_OF_TRAININGS):\n",
    "    # Train the agent\n",
    "    reinforce_baseline_policy_nn, episode_reward_list = (\n",
    "        train_reinforce_baseline_discrete(\n",
    "            env=env,\n",
    "            num_train_episodes=150,\n",
    "            num_test_per_episode=5,\n",
    "            max_episode_duration=500,\n",
    "            policy_learning_rate=0.005,\n",
    "            value_learning_rate=0.1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    reinforce_baseline_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
    "    reinforce_baseline_trains_result_list[1].extend(episode_reward_list)\n",
    "    reinforce_baseline_trains_result_list[2].extend(\n",
    "        [train_index for _ in episode_reward_list]\n",
    "    )\n",
    "\n",
    "reinforce_baseline_trains_result_df = pd.DataFrame(\n",
    "    np.array(reinforce_baseline_trains_result_list).T,\n",
    "    columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"],\n",
    ")\n",
    "reinforce_baseline_trains_result_df[\"agent\"] = \"REINFORCE baseline\"\n",
    "\n",
    "# Save the action-value estimation function of the last train\n",
    "\n",
    "torch.save(\n",
    "    reinforce_baseline_policy_nn,\n",
    "    MODELS_DIR / \"lab6_reinforce_with_baseline_policy_network.pth\",\n",
    ")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5282127f",
   "metadata": {},
   "source": [
    "#### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81523937",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    x=\"num_episodes\",\n",
    "    y=\"mean_final_episode_reward\",\n",
    "    kind=\"line\",\n",
    "    hue=\"agent\",\n",
    "    estimator=None,\n",
    "    units=\"training_index\",\n",
    "    data=reinforce_baseline_trains_result_df,\n",
    "    height=7,\n",
    "    aspect=2,\n",
    "    alpha=0.5,\n",
    ")\n",
    "plt.savefig(PLOTS_DIR / \"lab6_reinforce_with_baseline_cartpole_trains_result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef722194",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    x=\"num_episodes\",\n",
    "    y=\"mean_final_episode_reward\",\n",
    "    hue=\"agent\",\n",
    "    kind=\"line\",\n",
    "    data=reinforce_baseline_trains_result_df,\n",
    "    height=7,\n",
    "    aspect=2,\n",
    ")\n",
    "plt.savefig(PLOTS_DIR / \"lab6_reinforce_with_baseline_cartpole_trains_result_agg.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d765ee1b",
   "metadata": {},
   "source": [
    "#### Test final policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2d4a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PREFIX_EX2_REINFORCE_WITH_BASELINE_TRAINED = \"lab6_reinforce_with_baseline_trained\"\n",
    "\n",
    "NUM_EPISODES = 3\n",
    "\n",
    "file_path_list = [\n",
    "    FIGS_DIR / f\"{VIDEO_PREFIX_EX2_REINFORCE_WITH_BASELINE_TRAINED}-episode-{episode_index}.mp4\"\n",
    "    for episode_index in range(NUM_EPISODES)\n",
    "]\n",
    "\n",
    "for file_path in file_path_list:\n",
    "    file_path.unlink(missing_ok=True)\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(\n",
    "    env,\n",
    "    video_folder=str(FIGS_DIR),\n",
    "    name_prefix=VIDEO_PREFIX_EX2_REINFORCE_WITH_BASELINE_TRAINED,\n",
    "    episode_trigger=lambda x: True,\n",
    ")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=NUM_EPISODES)\n",
    "\n",
    "for episode_index in range(NUM_EPISODES):\n",
    "    episode_states, episode_actions, episode_rewards, episode_log_prob_actions = ... # TODO\n",
    "\n",
    "print(f\"Episode time taken: {env.time_queue}\")\n",
    "print(f\"Episode total rewards: {env.return_queue}\")\n",
    "print(f\"Episode lengths: {env.length_queue}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"\\nSelect the episode to play here ðŸ‘‡\\n\")\n",
    "\n",
    "interact(video_selector, file_path=file_path_list);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613686ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce_baseline_trains_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36464ade",
   "metadata": {},
   "source": [
    "#### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb352bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_ex2 = reinforce_baseline_trains_result_df[[\"num_episodes\", \"mean_final_episode_reward\"]].groupby(\"num_episodes\").mean().max()\n",
    "score_ex2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
