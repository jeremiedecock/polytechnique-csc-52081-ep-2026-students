{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1df705c",
   "metadata": {},
   "source": [
    "# CSC-52081-EP Lab4: Reinforcement Learning - TD Learning, QLearning and SARSA\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2026-students/refs/heads/main/assets/logo.jpg\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[CSC-52081-EP-2026](https://moodle.ip-paris.fr/course/view.php?id=10691) Lab session #4\n",
    "\n",
    "2019-2026 JÃ©rÃ©mie Decock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab28542",
   "metadata": {},
   "source": [
    "[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jeremiedecock/polytechnique-csc-52081-ep-2026-students/blob/main/lab4_rl2_tdlearning_qlearning_sarsa.ipynb)\n",
    "\n",
    "[![My Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jeremiedecock/polytechnique-csc-52081-ep-2026-students/main?filepath=lab4_rl2_tdlearning_qlearning_sarsa.ipynb)\n",
    "\n",
    "[![NbViewer](https://raw.githubusercontent.com/jupyter/design/main/logos/Badges/nbviewer_badge.svg)](https://nbviewer.jupyter.org/github/jeremiedecock/polytechnique-csc-52081-ep-2026-students/blob/main/lab4_rl2_tdlearning_qlearning_sarsa.ipynb)\n",
    "\n",
    "[![Local](https://img.shields.io/badge/Local-Save%20As...-blue)](https://github.com/jeremiedecock/polytechnique-csc-52081-ep-2026-students/raw/main/lab4_rl2_tdlearning_qlearning_sarsa.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4a5ad5",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The purpose of this lab is to introduce some classic concepts used\n",
    "in reinforcement learning: *Temporal Difference Learning* (*TD Learning*), *QLearning* and *SARSA*.\n",
    "\n",
    "As in previous labs, you can either:\n",
    "- open, edit and execute the notebook in *Google Colab* following this link: https://colab.research.google.com/github/jeremiedecock/polytechnique-csc-52081-ep-2026-students/blob/main/lab4_rl2_tdlearning_qlearning_sarsa.ipynb ; this is the **recommended** choice as you have nothing to install on your computer\n",
    "- open, edit and execute the notebook in *MyBinder* (if for any reason the Google Colab solution doesn't work): https://mybinder.org/v2/gh/jeremiedecock/polytechnique-csc-52081-ep-2026-students/main?filepath=lab4_rl2_tdlearning_qlearning_sarsa.ipynb\n",
    "- download, edit and execute the notebook on your computer if Python3 and JupyterLab are already installed: https://github.com/jeremiedecock/polytechnique-csc-52081-ep-2026-students/raw/main/lab4_rl2_tdlearning_qlearning_sarsa.ipynb\n",
    "\n",
    "If you work with Google Colab or MyBinder, **remember to save or download your work regularly or you may lose it!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8807b571-eeaa-47af-ad48-1ff4eeed988f",
   "metadata": {},
   "source": [
    "## Lab Submission\n",
    "\n",
    "Please submit your completed notebook in [Moodle : \"Lab 4 - Submission\"](https://moodle.ip-paris.fr/mod/assign/view.php?id=185079).\n",
    "\n",
    "### Submission Guidelines\n",
    "\n",
    "1. **File Naming:** Rename your notebook as follows: **`firstname_lastname-04.ipynb`** where `firstname` and `lastname` match your email address. *Example: `jesse_read-04.ipynb`*\n",
    "2. **Clear Output Cells:** To reduce file size (**must be under 500 KB**), clear all output cells before submitting. This includes rendered images, videos, plots, and dataframes...\n",
    "   - **JupyterLab:**\n",
    "     - Click **\"Kernel\" â†’ \"Restart Kernel and Clear Outputs of All Cells...\"**\n",
    "     - Then go to **\"File\" â†’ \"Save Notebook As...\"**\n",
    "   - **Google Colab:**\n",
    "     - Click **\"Edit\" â†’ \"Clear all outputs\"**\n",
    "     - Then go to **\"File\" â†’ \"Download\" â†’ \"Download.ipynb\"**\n",
    "   - **VSCode:**\n",
    "     - Click **\"Clear All Outputs\"**\n",
    "     - Then **save your file**\n",
    "3. **Upload Your File:** Only **`.ipynb`** files are accepted.\n",
    "\n",
    "**Note:** Bonus parts (if any) are optional, as their name suggests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe64cd6-4b8e-4328-a886-f14ec8cf4823",
   "metadata": {},
   "source": [
    "## Setup the Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3000eb-18fe-4c71-a273-2c5827bb2f30",
   "metadata": {},
   "source": [
    "This notebook relies on several libraries including `gymnasium[toy-text]` (v1.0.0), `ipywidgets`, `matplotlib`, `moviepy`, `numpy`, `pandas`, `pygame`, `seaborn`, and `tqdm`.\n",
    "A complete list of dependencies can be found in the following [requirements-lab4.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2026-students/main/requirements-lab4.txt) file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8289ea7c-3c1b-476c-94e1-9e3db57989c9",
   "metadata": {},
   "source": [
    "### If you use Google Colab\n",
    "\n",
    "If you use Google Colab, execute the next cell to install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d70dd97-a764-4245-8386-fcb0f27b938b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xt66Z85AmOI2",
    "outputId": "bd7a6b75-ad3c-4be1-d560-8239fbc0e9d2"
   },
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "\n",
    "def is_colab():\n",
    "    return \"google.colab\" in sys.modules\n",
    "\n",
    "def run_subprocess_command(cmd):\n",
    "    # run the command\n",
    "    process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
    "    # print the output\n",
    "    for line in process.stdout:\n",
    "        print(line.decode().strip())\n",
    "\n",
    "if is_colab():\n",
    "    # run_subprocess_command(\"apt install xvfb x11-utils\")\n",
    "    run_subprocess_command(\"pip install -r https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2026-students/main/requirements-lab4-google-colab.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7c2e10-dbd5-4932-8aa3-bb365c228267",
   "metadata": {},
   "source": [
    "### If you have downloaded the notebook on your computer and execute it in your own Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e3f3d5-2396-4abf-a57c-0497bea5ee79",
   "metadata": {},
   "source": [
    "To set up the necessary dependencies, run the following commands to establish a [Python virtual environment (venv)](https://docs.python.org/3/library/venv.html) that includes all the essential libraries for this lab.\n",
    "\n",
    "#### On POSIX systems (Linux, MacOSX, WSL, ...)\n",
    "\n",
    "```bash\n",
    "python3 -m venv env-lab4\n",
    "source env-lab4/bin/activate\n",
    "python3 -m pip install --upgrade pip\n",
    "python3 -m pip install -r https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2026-students/main/requirements-lab4.txt\n",
    "```\n",
    "\n",
    "#### On Windows\n",
    "\n",
    "```bash\n",
    "python3 -m venv env-lab4\n",
    "env-lab4\\Scripts\\activate.bat\n",
    "python3 -m pip install --upgrade pip\n",
    "python3 -m pip install -r https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2026-students/main/requirements-lab4.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463a4ef7-7e93-46a0-8501-8db18eb45b83",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e519bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from typing import cast, List, Optional, Sequence, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbc95c6-84bb-4236-bc5b-7d4ecb2be53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec9c23a-14f9-4d87-ab05-ec5cd2c83035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7fd184-1c69-4c10-966e-594f349c8471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f3f924",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d027660-4254-4298-992c-24d649163333",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGS_DIR = Path(\"figs/\") / \"lab4\"       # Where to save figures (.gif or .mp4 files)\n",
    "PLOTS_DIR = Path(\"figs/\") / \"lab4\"      # Where to save plots (.png or .svg files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2d37e8-adb7-4651-8ef4-25c763e3aaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not FIGS_DIR.exists():\n",
    "    FIGS_DIR.mkdir(parents=True)\n",
    "if not PLOTS_DIR.exists():\n",
    "    PLOTS_DIR.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cd6331-30c0-4fd7-8a6c-678d4e297236",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Define the video selector widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f4b588-1f58-4959-9f88-c93445f65810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_selector(file_path: List[Path]):\n",
    "    return Video(file_path, embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334a1eab",
   "metadata": {},
   "source": [
    "## Setup the FrozenLake toy problem with Gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd010fb",
   "metadata": {},
   "source": [
    "For the purpose of focusing on the algorithms, we will use standard environments provided\n",
    "by the Gymnasium framework. Especially, as in Lab 3, we will try to solve the FrozenLake-v1\n",
    "problem (https://gymnasium.farama.org/environments/toy_text/frozen_lake/).\n",
    "As a reminder, this environment is described [here](https://gymnasium.farama.org/environments/toy_text/frozen_lake/).\n",
    "\n",
    "The action indices are outlined below:\n",
    "\n",
    "| Action Index | Action     |\n",
    "|--------------|------------|\n",
    "| 0            | Move Left  |\n",
    "| 1            | Move Down  |\n",
    "| 2            | Move Right |\n",
    "| 3            | Move Up    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495c5593",
   "metadata": {},
   "source": [
    "The following dictionary may be used to understand actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf072d9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "action_labels = {0: \"Move Left\", 1: \"Move Down\", 2: \"Move Right\", 3: \"Move Up\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3173772",
   "metadata": {},
   "source": [
    "**Notice**: this environment is *fully observable*, thus here the terms (environment) *state* and (agent) *observation* are equivalent.\n",
    "This is not always the case for example in poker, the agent doesn't know the opponent's cards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1c5fbc",
   "metadata": {},
   "source": [
    "### Display functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55762df7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "The next cells contain three functions that can be used to display Q-tables, states and (greedy) policies in the FrozenLake environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435e02df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Q-table as a set of heatmaps, one for each action\n",
    "def display_qtable(\n",
    "    q_array: np.ndarray,\n",
    "    title: str = \"\",\n",
    "    figsize: Tuple[int, int] = (4, 4),\n",
    "    annot: bool = True,\n",
    "    fmt: str = \"0.1f\",\n",
    "    linewidths: float = 0.5,\n",
    "    square: bool = True,\n",
    "    cbar: bool = False,\n",
    "    cmap: str = \"Reds\",\n",
    "    ticklabels: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Display a Q-table as a set of heatmaps, one for each action.\n",
    "\n",
    "    For the frozen lake environment, there are 16 states and 4 actions thus this function will display 4 heatmaps, one for each action.\n",
    "    Each heatmap will display the Q-values for each state when performing the action indexed by the heatmap.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    q_array : np.ndarray\n",
    "        The Q-table to display. A 2D numpy array of 16x4 elements.\n",
    "        Each row corresponds to a state, and each column corresponds to an action.\n",
    "        In the frozen lake environment, there are 16 states and 4 actions thus the Q-table has a shape of (16, 4).\n",
    "        For instance, q_array[0, 3] is the Q-value (estimation of the cumulative expected reward) for performing action 3 (\"move up\") in state 0 (the top left square).\n",
    "    title : str, optional\n",
    "        The title of the plot. Default is an empty string.\n",
    "    figsize : tuple, optional\n",
    "        The size of the figure (in inches), by default (4, 4)\n",
    "    annot : bool, optional\n",
    "        If True, write the data value in each cell, by default True\n",
    "    fmt : str, optional\n",
    "        The string formatting code to use when adding annotations, by default \"0.1f\" that will display a single decimal\n",
    "    linewidths : float, optional\n",
    "        The width of the lines that will divide each cell, by default .5\n",
    "    square : bool, optional\n",
    "        Whether to set the Axes aspect to \"equal\" so each cell is square-shaped, by default True\n",
    "    cbar : bool, optional\n",
    "        Whether to draw a colorbar, by default False\n",
    "    cmap : str, optional\n",
    "        The mapping from data values to color space, by default \"Reds\".\n",
    "    ticklabels : bool, optional\n",
    "        If True, display the tick labels. Default is False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Get the number of actions from the shape of the Q-table\n",
    "    num_actions = q_array.shape[1]\n",
    "\n",
    "    # Adjust the figure size (in inches) based on the number of actions\n",
    "    global_figsize = list(figsize)\n",
    "    global_figsize[0] *= num_actions\n",
    "\n",
    "    # Create a subplot for each action\n",
    "    fig, ax_list = plt.subplots(ncols=num_actions, figsize=global_figsize)\n",
    "\n",
    "    # For each action, display the Q-values for all states as a heatmap\n",
    "    for action_index in range(num_actions):\n",
    "        ax = ax_list[action_index]\n",
    "\n",
    "        # Retrieve the Q-values for each state when performing the action indexed by \"action_index\".\n",
    "        # This forms a 1D array, state_vec, where state_vec[i] = Q(i, action_index).\n",
    "        state_vec = q_array[:, action_index]\n",
    "\n",
    "        # Display the Q-values for each state when performing the action indexed by \"action_index\"\n",
    "        # i.e. display Q(., action_index)\n",
    "        display_state(\n",
    "            state_vec,\n",
    "            title=r\"$Q(\\cdot,a_{})$\".format(action_index),\n",
    "            # title=r\"$Q(\\cdot,a_{})$ {}\".format(action_index, action_labels[action_index]),\n",
    "            figsize=figsize,\n",
    "            annot=annot,\n",
    "            fmt=fmt,\n",
    "            linewidths=linewidths,\n",
    "            square=square,\n",
    "            cbar=cbar,\n",
    "            cmap=cmap,\n",
    "            ticklabels=ticklabels,\n",
    "            ax=ax,\n",
    "        )\n",
    "\n",
    "    # Set the title for the entire figure\n",
    "    plt.suptitle(title)\n",
    "    # Display the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a2915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_state(\n",
    "    state_seq: Union[List[int], List[float], np.ndarray],\n",
    "    title: str = \"\",\n",
    "    figsize: Tuple[int, int] = (5, 5),\n",
    "    annot: bool = True,\n",
    "    fmt: str = \"0.1f\",\n",
    "    linewidths: float = 0.5,\n",
    "    square: bool = True,\n",
    "    cbar: bool = False,\n",
    "    cmap: str = \"Reds\",\n",
    "    ticklabels: bool = False,\n",
    "    ax: Optional[matplotlib.axes.Axes] = None,\n",
    ") -> Union[matplotlib.axes.Axes, None]:\n",
    "    \"\"\"\n",
    "    Display the expected values of all states as a heatmap.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state_seq : list of int, list of float or 1D numpy array of 16 elements\n",
    "        The sequence of expected values to display. This can be a list, a 1D array, etc.\n",
    "        Each element is the estimation of the expected value of the corresponding state.\n",
    "        For example, state_seq[0] is the estimation of the expected value of the first state.\n",
    "        There are 16 elements in this sequence for the frozenlake environment, i.e., one per state of the environment.\n",
    "    title : str, optional\n",
    "        The title of the plot, by default an empty string\n",
    "    figsize : tuple, optional\n",
    "        The size of the figure (in inches), by default (5, 5)\n",
    "    annot : bool, optional\n",
    "        If True, write the data value in each cell, by default True\n",
    "    fmt : str, optional\n",
    "        The string formatting code to use when adding annotations, by default \"0.1f\"\n",
    "    linewidths : float, optional\n",
    "        The width of the lines that will divide each cell, by default .5\n",
    "    square : bool, optional\n",
    "        Whether to set the Axes aspect to \"equal\" so each cell is square-shaped, by default True\n",
    "    cbar : bool, optional\n",
    "        Whether to draw a colorbar, by default False\n",
    "    cmap : str, optional\n",
    "        The mapping from data values to color space, by default \"Reds\".\n",
    "    ticklabels : bool, optional\n",
    "        If True, display the tick labels. Default is False.\n",
    "    ax : matplotlib.axes.Axes, optional\n",
    "        The axes object to draw the heatmap on, by default None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.axes.Axes, optional\n",
    "        The axes object with the heatmap if one was provided, otherwise None.\n",
    "    \"\"\"\n",
    "    # Calculate the size of the state array\n",
    "    size = int(math.sqrt(len(state_seq)))\n",
    "\n",
    "    # Convert the state sequence to a numpy array (if it isn't already one)\n",
    "    state_array = np.array(state_seq)\n",
    "\n",
    "    # Reshape the state array into a square matrix\n",
    "    # (we assume here that the original frozen lake environment is used,\n",
    "    # thus the state space can be visualized as a square grid)\n",
    "    state_array = state_array.reshape(size, size)\n",
    "\n",
    "    # If no axes object is provided, create a new figure and axes\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    # Create a heatmap of the state array on the axes\n",
    "    ax = sns.heatmap(\n",
    "        state_array,\n",
    "        annot=annot,\n",
    "        fmt=fmt,\n",
    "        linewidths=linewidths,\n",
    "        square=square,\n",
    "        cbar=cbar,\n",
    "        cmap=cmap,\n",
    "        xticklabels=ticklabels,\n",
    "        yticklabels=ticklabels,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    # If a title is provided, set the title of the plot\n",
    "    if title != \"\":\n",
    "        if ax is None:\n",
    "            plt.title(title)\n",
    "        else:\n",
    "            ax.set_title(title)\n",
    "\n",
    "    # If no axes object was provided, display the plot\n",
    "    # Otherwise, return the axes object with the heatmap\n",
    "    if ax is None:\n",
    "        plt.show()\n",
    "        return None\n",
    "    else:\n",
    "        return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c532444-f0e5-4335-a22d-aa7a69bd069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_policy(policy: List[int], environment: gym.Env) -> None:\n",
    "    \"\"\"\n",
    "    Display the policy as a heatmap.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    policy : list of int\n",
    "        The policy to be displayed. Each integer represents an action to be taken in a state.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Create a list of actions with their corresponding labels\n",
    "    actions_src = [\n",
    "        f\"{action}={action_labels[action].replace('Move ', '')}\" for action in range(environment.action_space.n)\n",
    "    ]\n",
    "\n",
    "    # Create a title for the heatmap using the actions and their labels\n",
    "    title = f\"Policy ({', '.join(actions_src)})\"\n",
    "\n",
    "    # Use the display_state function to create a heatmap of the policy\n",
    "    # The fmt parameter is set to \"d\" to display integers, cbar is set to False to not display a colorbar,\n",
    "    # and cmap is set to \"Reds\" to use the Reds color map\n",
    "    display_state(policy, title=title, fmt=\"d\", cbar=False, cmap=\"Reds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81339370",
   "metadata": {},
   "source": [
    "## Notation Table (common to TD Learning, SARSA, and Q-Learning)\n",
    "\n",
    "| Symbol                           | Meaning                                                                            |\n",
    "| -------------------------------- | ---------------------------------------------------------------------------------- |\n",
    "| $\\mathcal{S}$                    | Set of all states of the MDP                                                       |\n",
    "| $\\mathcal{S}^F$                  | Set of final (terminal) states                                                     |\n",
    "| $\\mathcal{S}^+$                  | Set of all non-final states, i.e. $\\mathcal{S} \\setminus \\mathcal{S}^F$            |\n",
    "| $\\mathcal{A}(s)$                 | Set of actions available in state $s$                                              |\n",
    "| $s, s'$                          | Current state and next state after a transition                                    |\n",
    "| $s_F$                            | A final (terminal) state                                                           |\n",
    "| $a, a'$                          | Current action and next action                                                     |\n",
    "| $r$                              | Immediate reward observed after taking an action                                   |\n",
    "| $\\pi(s)$                         | Policy: mapping from a state to an action                                          |\n",
    "| $\\epsilon$                       | Exploration parameter for $\\epsilon$-greedy action selection                       |\n",
    "| $\\gamma \\in [0,1]$               | Discount factor                                                                    |\n",
    "| $\\alpha \\in (0,1]$               | Learning rate (step size)                                                          |\n",
    "| $V(s)$                           | Estimated value of state $s$ under policy $\\pi$                                    |\n",
    "| $Q(s,a)$                         | Estimated action-value of taking action $a$ in state $s$                           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeef735",
   "metadata": {},
   "source": [
    "## Exercise 1: TD Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fece41-6205-4eb7-a4b9-cc03ff272b92",
   "metadata": {},
   "source": [
    "**Notice**: Here we assume that the reward only depends on the state: $r(\\boldsymbol{s}) \\equiv r(\\boldsymbol{s}, \\boldsymbol{a}, \\boldsymbol{s}')$.\n",
    "\n",
    "**Notice**:\n",
    "-  $\\mathcal{S}$ is the set of all nonterminal states\n",
    "-  $\\mathcal{S^F}$ is the set of all terminal states\n",
    "-  $\\mathcal{S^+}$ is the set of all states, including the terminal states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180195e1",
   "metadata": {},
   "source": [
    "In Lab3, we explored Dynamic Programming methods that can be used to solve a Markov Decision Process (MDP) when the environment is perfectly known to the agent, i.e., in cases where the agent knows the transition and reward functions in advance. However, this is a strong assumption, as in most practical problems, these functions are not known beforehand. In this lab, we will learn how to create agents that can solve Markov Decision Problems without prior knowledge of the environment. These agents learn the dynamics of their environment by exploring it and use this knowledge to find an optimal policy.\n",
    "\n",
    "We will start with the *TD Learning* (*Temporal Difference Learning*) algorithm, which can be used to **evaluate** any **given policy** (i.e. compute the *value function* of the environment following the given policy).\n",
    "Variants of TD Learning can also be adapted for optimal control, such as in applications like [TD-Gammon](https://en.wikipedia.org/wiki/TD-Gammon).\n",
    "Exercises 2 and 3 also reuse the main concepts of TD Learning to calculate an optimal policy.\n",
    "\n",
    "The algorithm is outlined below.\n",
    "\n",
    "---\n",
    "\n",
    "### The TD Learning algorithm\n",
    "\n",
    "<b>Input</b>:<br>\n",
    "\t$\\quad\\quad$ the policy $\\pi$ to be evaluated<br>\n",
    "<b>Algorithm parameter</b>:<br>\n",
    "\t$\\quad\\quad$ discount factor $\\gamma$<br>\n",
    "\t$\\quad\\quad$ step size $\\alpha \\in ]0,1]$<br><br>\n",
    "\n",
    "Initialize $V(\\boldsymbol{s})$ arbitrarily $\\forall \\boldsymbol{s} \\in \\mathcal{S^+}$, except that $V(\\boldsymbol{s}_F) \\leftarrow 0 ~ \\forall \\boldsymbol{s}_F \\in \\mathcal{S^F}$ (final states)<br>\n",
    "\n",
    "<b>FOR EACH</b> episode<br>\n",
    "\t$\\quad$ $\\boldsymbol{s} \\leftarrow \\text{env.reset}() \\quad\\quad\\quad\\quad\\quad\\quad~~$ <span style=\"color: gray;\">ðŸ‘ˆ initialize ***s***</span><br>\n",
    "\t$\\quad$ <b>DO</b> <br>\n",
    "\t\t$\\quad\\quad$ $\\boldsymbol{a} \\leftarrow \\pi(\\boldsymbol{s})$<br>\n",
    "\t\t$\\quad\\quad$ $\\boldsymbol{s}', r \\leftarrow \\text{env.step}(\\boldsymbol{a}) \\quad\\quad\\quad\\quad$ <span style=\"color: gray;\">ðŸ‘ˆ take action ***a***, observe *r* and ***s'***</span><br>\n",
    "\t\t$\\quad\\quad$ $V(\\boldsymbol{s}) \\leftarrow V(\\boldsymbol{s}) + \\alpha \\left[ \\underbrace{\\overbrace{r + \\gamma ~ V(\\boldsymbol{s}')}^{\\text{Target for } V(\\boldsymbol{s})} ~ - ~ V(\\boldsymbol{s})}_{\\text{TD error}} \\right]$<br>\n",
    "\t\t$\\quad\\quad$ $\\boldsymbol{s} \\leftarrow \\boldsymbol{s}'$<br>\n",
    "\t$\\quad$ <b>UNTIL</b> $\\boldsymbol{s}$ is final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2666142",
   "metadata": {},
   "source": [
    "**Notice**: in the following cell, `policy` is a list of actions (one per state see two cells bellow)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c264f90f-b643-4e7d-9153-01b8e49b10e5",
   "metadata": {},
   "source": [
    "### Task 1: Implement the TD Learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05aaf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY_EVERY_N_EPISODES = 50\n",
    "\n",
    "def td_learning(\n",
    "    policy: Sequence[int],\n",
    "    environment: gym.Env,\n",
    "    alpha: float = 0.1,\n",
    "    alpha_factor: float = 0.995,\n",
    "    gamma: float = 0.95,\n",
    "    num_episodes: int = 1000,\n",
    "    display: bool = False,\n",
    ") -> Tuple[np.ndarray, List[np.ndarray], List[float]]:\n",
    "    \"\"\"\n",
    "    Perform Temporal Difference learning on a given policy and environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    policy : Sequence[int]\n",
    "        The policy to be learned, represented as a sequence mapping states (the index of the sequence) to actions (the value of the sequence for this index).\n",
    "        For example, policy[0] is the action to take in state 0.\n",
    "    environment : gym.Env\n",
    "        The environment in which the agent operates.\n",
    "    alpha : float, optional\n",
    "        The learning rate, between 0 and 1. By default 0.1\n",
    "    alpha_factor : float, optional\n",
    "        The factor by which the learning rate alpha decreases each episode, by default 0.995\n",
    "    gamma : float, optional\n",
    "        The discount factor, between 0 and 1. By default 0.95\n",
    "    num_episodes : int, optional\n",
    "        The number of episodes to run, by default 1000\n",
    "    display : bool, optional\n",
    "        Whether to display the value function (every DISPLAY_EVERY_N_EPISODES episodes), by default False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[np.ndarray, List[np.ndarray], List[float]]\n",
    "        A tuple containing:\n",
    "        - The learned value function (1D ndarray of shape (16,))\n",
    "        - The history of value functions over episodes\n",
    "        - The history of alpha values over episodes\n",
    "    \"\"\"\n",
    "    # Initialize the history of the value function and the learning rate\n",
    "    v_array_history: List[np.ndarray] = []\n",
    "    alpha_history: List[float] = []\n",
    "\n",
    "    observation_space = cast(gym.spaces.Discrete, environment.observation_space)\n",
    "\n",
    "    # Get the number of states in the environment\n",
    "    num_states = observation_space.n\n",
    "\n",
    "    # Initialize the value function to zeros\n",
    "    v_array = np.zeros(num_states)\n",
    "\n",
    "    # Loop over the episodes\n",
    "    for episode_index in tqdm(range(1, num_episodes)):\n",
    "        # Display the value function every DISPLAY_EVERY_N_EPISODES episodes if display is True\n",
    "        if display and episode_index % DISPLAY_EVERY_N_EPISODES == 0:\n",
    "            display_state(\n",
    "                v_array,\n",
    "                title=f\"Value function (ep. {episode_index})\",\n",
    "                cbar=True,\n",
    "                cmap=\"Reds\",\n",
    "            )\n",
    "\n",
    "        # Save the current value function and learning rate\n",
    "        v_array_history.append(v_array.copy())\n",
    "        alpha_history.append(alpha)\n",
    "\n",
    "        # Decrease the learning rate if alpha_factor is not None\n",
    "        if alpha_factor is not None:\n",
    "            alpha = alpha * alpha_factor\n",
    "\n",
    "        # TODO ðŸ‘ˆ Complete the implementation of the Temporal-Difference (TD) algorithm\n",
    "\n",
    "    # Return the learned value function\n",
    "    return v_array, v_array_history, alpha_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fddba4b",
   "metadata": {},
   "source": [
    "**Note**: In the following cell, the `display` argument can be set to `True` to see the evolution of the value function `v_array` over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9498322c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Define the policy to evaluate\n",
    "policy = [0, 3, 3, 3,\n",
    "          0, 0, 0, 0,\n",
    "          3, 1, 0, 0,\n",
    "          0, 2, 1, 0]\n",
    "\n",
    "# Create the environment and set the maximum number of steps per episode\n",
    "environment = gym.make(\"FrozenLake-v1\", max_episode_steps=1000)\n",
    "\n",
    "# Apply Temporal Difference (TD) Learning to calculate the value function for the policy defined earlier, within the context of the FrozenLake environment.\n",
    "v_array_ex1, v_array_history_ex1, alpha_history_ex1 = td_learning(policy, environment, display=False)\n",
    "\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41a47b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the learned value function\n",
    "display_state(v_array_ex1, title=\"Value function\", cbar=True, cmap=\"Reds\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caa3bc2",
   "metadata": {},
   "source": [
    "### Display the evolution of the action-value function over iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139261c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the evolution of the value function during the learning process\n",
    "df_v_hist_ex1 = pd.DataFrame(v_array_history_ex1)\n",
    "df_v_hist_ex1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdee110",
   "metadata": {},
   "source": [
    "Evolution of `v_array_ex1` (the estimated expected value of each state) over iterations (one curve per state):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74da2d6a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Show the evolution of the estimated expected value for each state during the learning process\n",
    "df_v_hist_ex1.plot(figsize=(14, 8))\n",
    "plt.title(r\"$V^{\\pi}(\\cdot)$ w.r.t iteration\")\n",
    "plt.ylabel(r\"$V^{\\pi}(\\cdot)$\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.legend(loc=\"upper right\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182f7b5e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Show the evolution of the alpha parameter during the learning process\n",
    "plt.loglog(alpha_history_ex1)\n",
    "plt.title(\"Alpha w.r.t iteration\")\n",
    "plt.ylabel(\"Alpha\")\n",
    "plt.xlabel(\"iteration\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e017a70d-f905-482d-b7c8-07cd09beff1c",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cdb39e-8a88-4365-987c-fa62cf1c3e08",
   "metadata": {},
   "source": [
    "Copy and paste the output of the following cell into the first question of the [Lab 4 - Evaluation](https://moodle.ip-paris.fr/mod/quiz/view.php?id=185078) in Moodle:  \n",
    "*\"What is the expected value of the 9th state (i.e. `v_array[8]`) in the FrozenLake environment for the given policy Ï€ using the default parameters?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf8f7a0-dc39-43e8-8e6d-88768ffb3e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "NUM_RUNS = 10\n",
    "\n",
    "# Define the policy to evaluate\n",
    "policy = [0, 3, 3, 3,\n",
    "          0, 0, 0, 0,\n",
    "          3, 1, 0, 0,\n",
    "          0, 2, 1, 0]\n",
    "\n",
    "# Create the environment and set the maximum number of steps per episode\n",
    "environment = gym.make(\"FrozenLake-v1\", max_episode_steps=1000)\n",
    "\n",
    "run_list = []\n",
    "for _ in tqdm(range(1, NUM_RUNS)):\n",
    "    v_array, _, _ = td_learning(policy, environment, display=False)\n",
    "    run_list.append(v_array[8].item())\n",
    "\n",
    "environment.close()\n",
    "\n",
    "print(f\"\\n\\nâš ï¸ Copy and paste this value into Moodle ðŸ‘‰ {np.mean(run_list).item():.2f}\\n(it has the required number of decimals)\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211142c8-2f16-4530-8843-5e77dd3a6b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â print(np.std(run_list))\n",
    "# pd.DataFrame(run_list).plot.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4245482",
   "metadata": {},
   "source": [
    "### Task 2: The learning rate $\\alpha$ in TD-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deadf82-0647-4a58-a87b-1a4f208672fa",
   "metadata": {},
   "source": [
    "In the previous exercise, set `alpha_factor` to 1 then check the algorithm with different `alpha` values between 0.1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae37af46-ca66-421e-9957-e6eb9130f0f5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Define the policy to evaluate\n",
    "policy = [0, 3, 3, 3,\n",
    "          0, 0, 0, 0,\n",
    "          3, 1, 0, 0,\n",
    "          0, 2, 1, 0]\n",
    "\n",
    "# Create the environment and set the maximum number of steps per episode\n",
    "environment = gym.make(\"FrozenLake-v1\", max_episode_steps=1000)\n",
    "\n",
    "# Apply Temporal Difference (TD) Learning to calculate the value function for the policy defined earlier, within the context of the FrozenLake environment.\n",
    "_, v_array_history, _ = td_learning(policy, environment, display=False, alpha_factor=1, alpha=0.5)  # ðŸ‘ˆ Test different `alpha` values here\n",
    "\n",
    "df = pd.DataFrame(v_array_history)\n",
    "\n",
    "# Show the evolution of the estimated expected value for each state during the learning process\n",
    "df.plot(figsize=(14, 8))\n",
    "plt.title(r\"$V^{\\pi}(\\cdot)$ w.r.t iteration\")\n",
    "plt.ylabel(r\"$V^{\\pi}(\\cdot)$\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.legend(loc=\"upper right\");\n",
    "\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5306051f-e912-4162-91c0-750a2d7a5c36",
   "metadata": {},
   "source": [
    "### Bonus question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce31afc-c2d2-4114-8aff-7ac6061a1460",
   "metadata": {},
   "source": [
    "What do you observe when you change the `alpha_factor` ?\n",
    "What is the role of this parameter ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999b07d2",
   "metadata": {},
   "source": [
    "## Exercise 2: SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04f74d6-3374-4951-8587-178cb13bbe26",
   "metadata": {},
   "source": [
    "### The Greedy and Epsilon Greedy policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa503e3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In exercise 1, **TD-Learning** has been used to **estimate the value function** for a **given policy**.  \n",
    "In the following exercises, we will now explore how to **find the optimal** (or a nearly optimal) policy.  \n",
    "To do that, we will use two algorithms (*SARSA* and *Q-Learning*) that estimate a *Q-table* instead of a *V-table*.  \n",
    "\n",
    "This *Q-table* provides the expected **cumulative reward** when the agent takes a given action $\\boldsymbol{a}$ from any state $\\boldsymbol{s}$ and then follows a specified *behavior policy* to choose subsequent actions until a terminal state is reached. (This behavior policy may also serve as an *exploration policy* guiding action selection.)  \n",
    "\n",
    "While exploring the environment, the agent **updates** its *Q-table* using a **rule derived from the Bellman equation** (i.e., an *update rule*).  \n",
    "\n",
    "The purpose of this exercise is to implement the *greedy* and the $\\epsilon$-*greedy* policies that agents will use to explore the environment and update their Q-Table:\n",
    "\n",
    "$\\displaystyle \\pi^{Q^{\\pi}}(\\boldsymbol{s}) := \\text{greedy}(\\boldsymbol{s}, Q^{\\pi}) = \\arg\\max_{\\boldsymbol{a} \\in \\mathcal{A}} Q^{\\pi}(\\boldsymbol{s}, \\boldsymbol{a})$\n",
    "\n",
    "\n",
    "$\\pi^{Q^{\\pi}}_{\\epsilon}(\\boldsymbol{s}) := \\epsilon\\text{-greedy}(\\boldsymbol{s}, Q^{\\pi}) = $\n",
    "randomly choose between $\\underbrace{\\text{greedy}(\\boldsymbol{s}, Q^{\\pi})}_{\\text{with probability } 1 - \\epsilon}$\n",
    "and $~~ \\underbrace{\\text{a random action}}_{\\text{with probability } \\epsilon}$    i.e. $\\epsilon \\in (0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2364ec0-7510-4105-96e0-6a921b20cf1d",
   "metadata": {},
   "source": [
    "### Task 1: Implement the Greedy and Epsilon Greedy policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e97141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(state: int, q_array: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Determine the action that maximizes the Q-value for a given state.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : int\n",
    "        The current state.\n",
    "    q_array : np.ndarray\n",
    "        The Q-table.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The action that maximizes the Q-value for the given state.\n",
    "    \"\"\"\n",
    "    # Find the action that maximizes the Q-value for the given state\n",
    "    action = ... # TODO ðŸ‘ˆ Complete the implementation of the greedy policy\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state: int, q_array: np.ndarray, epsilon: float) -> int:\n",
    "    \"\"\"\n",
    "    Determine the action to take based on an epsilon-greedy policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : int\n",
    "        The current state.\n",
    "    q_array : np.ndarray\n",
    "        The Q-table.\n",
    "    epsilon : float\n",
    "        The probability of choosing a random action.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The action to take.\n",
    "    \"\"\"\n",
    "    # TODO ðŸ‘ˆ Complete the implementation of the epsilon greedy policy\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c71a5a2",
   "metadata": {},
   "source": [
    "### The SARSA algorithm\n",
    "\n",
    "To find the optimal policy (or a nearly optimal policy) for the FrozenLake-v1 problem, we will first use the SARSA algorithm.\n",
    "It is based on the online update of the *QTable* for the current policy, defined as:\n",
    "$$\n",
    "Q(s, a) = \\mathbb{E}^{\\pi} \\left[ \\sum_{t=0}^{H} \\gamma^t r_{t}(s_t, a_t) | s_t=s_0, a=a_0 \\right] ,\n",
    "$$\n",
    "where $\\gamma \\in (0, 1]$ is the discount factor, and $H$ the horizon of the episode.\n",
    "\n",
    "The SARSA algorithm updates a tabular estimate of the Q-function using the following update rule:\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) +\n",
    "\\alpha\n",
    "\\left[\n",
    "\\underbrace{\\overbrace{r + \\gamma Q(s', a')}^{\\text{Target}} - Q(s, a)}_{\\text{TD error}}\n",
    "\\right] ,\n",
    "$$\n",
    "where $\\alpha \\in (0, 1]$ is the learning rate, and $r_t$ is the reward received by the agent at time step $t$.\n",
    "Most of the time, the SARSA algorithm is implemented with an $\\epsilon$-greedy exploration strategy.\n",
    "This strategy consists in selecting the best action learned so far with probability $(1 - \\epsilon)$ and to select a random\n",
    "action with probability $\\epsilon$.\n",
    "\n",
    "---\n",
    "\n",
    "***The SARSA algorithm***\n",
    "\n",
    "<b>Input</b>:<br>\n",
    "\t$\\quad\\quad$ none<br>\n",
    "<b>Algorithm parameter</b>:<br>\n",
    "\t$\\quad\\quad$ discount factor $\\gamma$<br>\n",
    "\t$\\quad\\quad$ step size $\\alpha \\in ]0,1]$<br>\n",
    "\t$\\quad\\quad$ small $\\epsilon > 0$<br><br>\n",
    "\n",
    "Initialize $Q(\\boldsymbol{s}, \\boldsymbol{a})$ arbitrarily $\\forall \\boldsymbol{s} \\in \\mathcal{S}, \\boldsymbol{a} \\in \\mathcal{A}(\\boldsymbol{s})$,\n",
    "except that $Q(\\boldsymbol{s}_F, \\cdot) \\leftarrow 0 ~~~ \\forall \\boldsymbol{s}_F \\in \\mathcal{S}^F$ (final states)<br><br>\n",
    "\n",
    "<b>FOR EACH</b> episode<br>\n",
    "\t$\\quad$ $\\boldsymbol{s} \\leftarrow \\text{env.reset}() \\quad\\quad\\quad\\quad\\quad\\quad~~$ <span style=\"color: gray;\">ðŸ‘ˆ initialize ***s***</span><br>\n",
    "\t$\\quad$ $\\boldsymbol{a} \\leftarrow \\epsilon\\text{-greedy}(\\boldsymbol{s}, Q)$<br>\n",
    "\t$\\quad$ <b>DO</b> <br>\n",
    "\t\t$\\quad\\quad$ $r, \\boldsymbol{s}' \\leftarrow \\text{env.step}(\\boldsymbol{a}) \\quad\\quad\\quad\\quad$ <span style=\"color: gray;\">ðŸ‘ˆ take action ***a***, observe *r* and ***s'***</span><br>\n",
    "\t\t$\\quad\\quad$ $\\boldsymbol{a}' \\leftarrow \\epsilon\\text{-greedy}(\\boldsymbol{s}', Q)$<br>\n",
    "\t\t$\\quad\\quad$ $Q(\\boldsymbol{s},\\boldsymbol{a}) \\leftarrow Q(\\boldsymbol{s},\\boldsymbol{a}) + \\alpha \\left[ \\underbrace{\\overbrace{r + \\gamma Q(s', a')}^{\\text{Target}} ~ - ~ Q(\\boldsymbol{s},\\boldsymbol{a})}_{\\text{TD error}} \\right]$<br>\n",
    "\t\t$\\quad\\quad$ $\\boldsymbol{s} \\leftarrow \\boldsymbol{s}'$<br>\n",
    "\t\t$\\quad\\quad$ $\\boldsymbol{a} \\leftarrow \\boldsymbol{a}'$<br>\n",
    "\t$\\quad$ <b>UNTIL</b> $\\boldsymbol{s}$ is final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e40900-fa16-471b-b35d-e5cf967851c1",
   "metadata": {},
   "source": [
    "### Task 2: Implement the SARSA algorithm\n",
    "\n",
    "**Tasks**: Implement the SARSA algorithm with $\\epsilon$-greedy exploration (start with $\\epsilon = 0.5$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5528fc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY_EVERY_N_EPISODES = 50\n",
    "\n",
    "def sarsa(\n",
    "    environment: gym.Env,\n",
    "    alpha: float = 0.1,\n",
    "    alpha_factor: float = 0.9995,\n",
    "    gamma: float = 0.99,\n",
    "    epsilon: float = 0.5,\n",
    "    num_episodes: int = 10000,\n",
    "    display: bool = False,\n",
    ") -> Tuple[np.ndarray, List[np.ndarray], List[float]]:\n",
    "    \"\"\"\n",
    "    Learn an action-value function Q(s, a) with the on-policy SARSA algorithm\n",
    "    using an epsilon-greedy behavior policy.\n",
    "\n",
    "    The update performed at each step is:\n",
    "        Q(s, a) <- Q(s, a) + alpha * (r + gamma * Q(s', a') - Q(s, a))\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    environment : gym.Env\n",
    "        Environment following the (Gymnasium) API:\n",
    "        - reset() -> (observation, info)\n",
    "        - step(action) -> (observation, reward, terminated, truncated, info)\n",
    "        The observation space and action space are assumed to be discrete.\n",
    "    alpha : float, optional\n",
    "        Initial learning rate in (0, 1]. Default is 0.1.\n",
    "    alpha_factor : float, optional\n",
    "        Multiplicative decay applied to `alpha` after each episode. If None,\n",
    "        `alpha` stays constant. Default is 0.9995.\n",
    "    gamma : float, optional\n",
    "        Discount factor in (0, 1]. Default is 0.99.\n",
    "    epsilon : float, optional\n",
    "        Epsilon for epsilon-greedy action selection (probability of random\n",
    "        action). Default is 0.5.\n",
    "    num_episodes : int, optional\n",
    "        Number of training episodes. Default is 10000.\n",
    "    display : bool, optional\n",
    "        If True, display the Q-table every `DISPLAY_EVERY_N_EPISODES` episodes.\n",
    "        Default is False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    q_array : np.ndarray\n",
    "        Learned Q-table of shape (num_states, num_actions).\n",
    "        For instance, for the FrozenLake environment, q_array[0, 3] is\n",
    "        the Q-value (estimation of the cumulative expected reward) for performing\n",
    "        action 3 (\"move up\") in state 0 (the top left square).\n",
    "    q_array_history : List[np.ndarray]\n",
    "        Snapshot of the Q-table taken once per episode (before training within\n",
    "        that episode).\n",
    "    alpha_history : List[float]\n",
    "        Value of `alpha` used for each episode (recorded before applying decay).\n",
    "    \"\"\"\n",
    "    # Initialize the history of the Q-table and learning rate\n",
    "    q_array_history = []\n",
    "    alpha_history = []\n",
    "\n",
    "    observation_space = cast(gym.spaces.Discrete, environment.observation_space)\n",
    "    action_space = cast(gym.spaces.Discrete, environment.action_space)\n",
    "\n",
    "    # Get the number of states in the environment\n",
    "    num_states = observation_space.n\n",
    "\n",
    "    # Get the number of actions in the environment\n",
    "    num_actions = action_space.n\n",
    "\n",
    "    # Initialize the Q-table to zeros\n",
    "    q_array = np.zeros([num_states, num_actions])\n",
    "\n",
    "    # Loop over the episodes\n",
    "    for episode_index in tqdm(range(1, num_episodes)):\n",
    "        # Display the Q-table every DISPLAY_EVERY_N_EPISODES episodes if display is True\n",
    "        if display and episode_index % DISPLAY_EVERY_N_EPISODES == 0:\n",
    "            display_qtable(q_array, title=\"Q table\")\n",
    "\n",
    "        # Save the current Q-table and learning rate\n",
    "        q_array_history.append(q_array.copy())\n",
    "        alpha_history.append(alpha)\n",
    "\n",
    "        # Decrease the learning rate if alpha_factor is not None\n",
    "        if alpha_factor is not None:\n",
    "            alpha = alpha * alpha_factor\n",
    "\n",
    "        # TODO ðŸ‘ˆ Complete the implementation of the SARSA algorithm\n",
    "\n",
    "    # Return the learned Q-table\n",
    "    return q_array, q_array_history, alpha_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceecb2c",
   "metadata": {},
   "source": [
    "**Note**: In the following cell, the `display` argument can be set to `True` to see the evolution of the action-value function `q_array` over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b910413",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create the environment and set the maximum number of steps per episode\n",
    "environment = gym.make(\"FrozenLake-v1\", max_episode_steps=1000)\n",
    "\n",
    "# Apply SARSA to calculate the Q-table for the FrozenLake environment\n",
    "q_array_ex2, q_array_history_ex2, alpha_history_ex2 = sarsa(environment, display=False)\n",
    "\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33158453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the learned Q-table\n",
    "display_qtable(q_array_ex2, title=\"Q Table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6108c1-b521-4df4-bad4-dfa78b6ff1ad",
   "metadata": {},
   "source": [
    "### Display the greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc94810-82fe-4d14-be4b-044d6672a7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_ex2 = [greedy_policy(state, q_array_ex2) for state in range(environment.observation_space.n)]\n",
    "policy_ex2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b9ef21-46a9-48a5-aa35-f331de5bab69",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_policy(policy_ex2, environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529a6db2",
   "metadata": {},
   "source": [
    "### Display the evolution of the action-value function over iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587239de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a Pandas dataframe containing the evolution of the Q-table during the learning process\n",
    "q_array_history_array_ex2 = np.array(q_array_history_ex2)\n",
    "df_q_hist_list_ex2 = []\n",
    "\n",
    "for action_index in range(q_array_history_array_ex2.shape[2]):\n",
    "    df_q_hist_list_ex2.append(pd.DataFrame(q_array_history_array_ex2[:, :, action_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec46f0df",
   "metadata": {},
   "source": [
    "Evolution of `q_array_ex2` over iterations (one curve per state):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da39d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the evolution of the estimated expected value for each action and state during the learning process\n",
    "for action_index, df_q_hist in enumerate(df_q_hist_list_ex2):\n",
    "    df_q_hist.plot(figsize=(14,8))\n",
    "    plt.title(r'$Q(\\cdot,a_{})$ w.r.t iteration with $a_{}$ := \"{}\"'.format(action_index, action_index, action_labels[action_index]))\n",
    "    plt.ylabel(r\"$Q(\\cdot,a_{})$\".format(action_index))\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae72d8be",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Show the evolution of the alpha parameter during the learning process\n",
    "plt.loglog(alpha_history_ex2)\n",
    "plt.title(\"Alpha w.r.t iteration\")\n",
    "plt.ylabel(\"Alpha\")\n",
    "plt.xlabel(\"iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c74025",
   "metadata": {},
   "source": [
    "### Evaluate Policy with Gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4403a6",
   "metadata": {},
   "source": [
    "As a measure of performance, count the number of successful trials on 1000 episodes.\n",
    "\n",
    "**Note**: Gymnasium considers the task is solved if you reach 76\\% of success over the episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be14ab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = gym.make(\"FrozenLake-v1\", max_episode_steps=1000)\n",
    "\n",
    "reward_list = []\n",
    "NUM_EPISODES = 1000\n",
    "\n",
    "for episode_index in range(NUM_EPISODES):\n",
    "    state, info = environment.reset()\n",
    "    episode_over = False\n",
    "\n",
    "    while not episode_over:\n",
    "        # action = epsilon_greedy_policy(state, q_array_ex2, epsilon)\n",
    "        action = greedy_policy(state, q_array_ex2)\n",
    "        state, reward, terminated, truncated, info = environment.step(action)\n",
    "        episode_over = terminated or truncated\n",
    "\n",
    "    reward_list.append(reward)\n",
    "\n",
    "reward_df = pd.DataFrame(reward_list)\n",
    "\n",
    "print('Average reward (which is equivalent to a \"success rate\" in the FrozenLake environment as the total rewards in this environment are either 0 or 1):', np.average(reward_df))\n",
    "\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7134c7d9-ec9e-41ad-a94f-4612f3c8528f",
   "metadata": {},
   "source": [
    "### Test the optimized policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb0a5f4-adc2-4ac0-8c78-dd2870b4c520",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PREFIX_EX2 = \"lab4_ex2_sarsa\"\n",
    "\n",
    "NUM_EPISODES = 3\n",
    "\n",
    "file_path_list = [\n",
    "    FIGS_DIR / f\"{VIDEO_PREFIX_EX2}-episode-{episode_index}.mp4\"\n",
    "    for episode_index in range(NUM_EPISODES)\n",
    "]\n",
    "\n",
    "for file_path in file_path_list:\n",
    "    file_path.unlink(missing_ok=True)\n",
    "\n",
    "environment = gym.make(\"FrozenLake-v1\", max_episode_steps=1000, render_mode=\"rgb_array\")\n",
    "environment = gym.wrappers.RecordVideo(\n",
    "    environment,\n",
    "    video_folder=str(FIGS_DIR),\n",
    "    name_prefix=VIDEO_PREFIX_EX2,\n",
    "    episode_trigger=lambda x: True\n",
    ")\n",
    "environment = gym.wrappers.RecordEpisodeStatistics(environment, buffer_length=NUM_EPISODES)\n",
    "\n",
    "for episode_index in range(NUM_EPISODES):\n",
    "    state, info = environment.reset()\n",
    "\n",
    "    episode_over = False\n",
    "    while not episode_over:\n",
    "        # action = epsilon_greedy_policy(state, q_array_ex2, epsilon)\n",
    "        action = greedy_policy(state, q_array_ex2)\n",
    "        state, reward, terminated, truncated, info = environment.step(action)\n",
    "        episode_over = terminated or truncated\n",
    "\n",
    "    reward_list.append(reward)\n",
    "\n",
    "print(f'Episode time taken: {environment.time_queue}')\n",
    "print(f'Episode total rewards: {environment.return_queue}')\n",
    "print(f'Episode lengths: {environment.length_queue}')\n",
    "\n",
    "environment.close()\n",
    "\n",
    "print(\"\\nSelect the episode to play here ðŸ‘‡\\n\")\n",
    "\n",
    "interact(video_selector, file_path=file_path_list);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fac14b-d308-4643-a64a-fdc591453583",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f694ddd-ed23-4339-a190-183b0f1635d5",
   "metadata": {},
   "source": [
    "Copy and paste the output of the following cell into the second question of the [Lab 4 - Evaluation](https://moodle.ip-paris.fr/mod/quiz/view.php?id=185078) in Moodle:\n",
    "*\"What is the Q-value Q(s0,a0) of the initial state and action 0 (\"Move Left\") in the FrozenLake environment using the default parameters?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce426e58-c589-402f-b043-93cb4c464eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "NUM_RUNS = 10\n",
    "\n",
    "# Create the environment and set the maximum number of steps per episode\n",
    "environment = gym.make(\"FrozenLake-v1\", max_episode_steps=1000)\n",
    "\n",
    "run_list = []\n",
    "for _ in tqdm(range(1, NUM_RUNS)):\n",
    "    q_array, _, _ = sarsa(environment, display=False)\n",
    "    run_list.append(q_array[0][0].item())\n",
    "\n",
    "environment.close()\n",
    "\n",
    "print(f\"\\n\\nâš ï¸ Copy and paste this value into Moodle ðŸ‘‰ {np.mean(run_list).item():.2f}\\n(it has the required number of decimals)\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af08049-baac-4f1c-acf6-e72e77faf15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.std(run_list))\n",
    "#Â pd.DataFrame(run_list).plot.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cab2ba",
   "metadata": {},
   "source": [
    "## Exercise 3: QLearning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e911d29",
   "metadata": {},
   "source": [
    "Another reinforcement learning algorithm is *Q-Learning*.\n",
    "The fundamental difference from *SARSA* is that it is an *off-policy* algorithm.\n",
    "This means it learns from trajectories generated by a policy different from the one it is optimizing.\n",
    "In other words, the *behavior policy* (*epsilon-greedy*) is different from the *target policy* (*greedy*).\n",
    "\n",
    "To do so, it uses the following update rule:\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) +\n",
    "\\alpha\n",
    "\\left[\n",
    "\\underbrace{\\overbrace{r + \\gamma \\max_{a^* \\in \\mathcal{A}} Q(s', a^*)}^{\\text{Target}} - Q(s, a)}_{\\text{TD error}}\n",
    "\\right] ,\n",
    "$$\n",
    "\n",
    "**Task**: in this exercise, you will replace the SARSA update rule by the Q-learning one and analyze the\n",
    "differences in performance.\n",
    "\n",
    "---\n",
    "\n",
    "### The QLearning algorithm\n",
    "\n",
    "<b>Input</b>:<br>\n",
    "\t$\\quad\\quad$ none<br>\n",
    "<b>Algorithm parameter</b>:<br>\n",
    "\t$\\quad\\quad$ discount factor $\\gamma$<br>\n",
    "\t$\\quad\\quad$ step size $\\alpha \\in ]0,1]$<br>\n",
    "\t$\\quad\\quad$ small $\\epsilon > 0$<br><br>\n",
    "\n",
    "Initialize $Q(\\boldsymbol{s}, \\boldsymbol{a})$ arbitrarily $\\forall \\boldsymbol{s} \\in \\mathcal{S}, \\boldsymbol{a} \\in \\mathcal{A}(\\boldsymbol{s})$,\n",
    "except that $Q(\\boldsymbol{s}_F, \\cdot) \\leftarrow 0 ~~~ \\forall \\boldsymbol{s}_F \\in \\mathcal{S}^F$ (final states)<br><br>\n",
    "\n",
    "<b>FOR EACH</b> episode<br>\n",
    "\t$\\quad$ $\\boldsymbol{s} \\leftarrow \\text{env.reset}() \\quad\\quad\\quad\\quad\\quad\\quad~~$ <span style=\"color: gray;\">ðŸ‘ˆ initialize ***s***</span><br>\n",
    "\t$\\quad$ <b>DO</b> <br>\n",
    "\t\t$\\quad\\quad$ $\\boldsymbol{a} \\leftarrow \\epsilon\\text{-greedy}(\\boldsymbol{s}, Q)$<br>\n",
    "\t\t$\\quad\\quad$ $r, \\boldsymbol{s}' \\leftarrow \\text{env.step}(\\boldsymbol{a}) \\quad\\quad\\quad\\quad$ <span style=\"color: gray;\">ðŸ‘ˆ take action ***a***, observe *r* and ***s'***</span><br>\n",
    "\t\t$\\quad\\quad$ $Q(\\boldsymbol{s},\\boldsymbol{a}) \\leftarrow Q(\\boldsymbol{s},\\boldsymbol{a}) + \\alpha \\left[ \\underbrace{\\overbrace{r + \\gamma ~ \\max_{\\boldsymbol{a^*} \\in \\mathcal{A}} Q(\\boldsymbol{s}', \\boldsymbol{a^*})}^{\\text{Target}} ~ - ~ Q(\\boldsymbol{s},\\boldsymbol{a})}_{\\text{TD error}} \\right]$<br>\n",
    "\t\t$\\quad\\quad$ $\\boldsymbol{s} \\leftarrow \\boldsymbol{s}'$<br>\n",
    "\t$\\quad$ <b>UNTIL</b> $\\boldsymbol{s}$ is final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13400eb8-378b-4415-a306-53775bb65f62",
   "metadata": {},
   "source": [
    "### Task 1: Implement the QLearning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d494d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY_EVERY_N_EPISODES = 50\n",
    "\n",
    "def q_learning(\n",
    "    environment: gym.Env,\n",
    "    alpha: float = 0.1,\n",
    "    alpha_factor: float = 0.9995,\n",
    "    gamma: float = 0.99,\n",
    "    epsilon: float = 0.5,\n",
    "    num_episodes: int = 10000,\n",
    "    display: bool = False,\n",
    ") -> Tuple[np.ndarray, List[np.ndarray], List[float]]:\n",
    "    \"\"\"\n",
    "    Train an agent with the tabular Q-learning algorithm (off-policy TD control).\n",
    "\n",
    "    The agent follows an epsilon-greedy *behavior* policy derived from the current Q-table,\n",
    "    while the update uses the greedy action in the next state (the *target* policy):\n",
    "\n",
    "        Q[s, a] <- Q[s, a] + alpha * (r + gamma * max_a' Q[s', a'] - Q[s, a])\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    environment : gym.Env\n",
    "        Gymnasium environment with discrete observation and action spaces\n",
    "        (i.e., ``environment.observation_space.n`` and ``environment.action_space.n`` exist).\n",
    "    alpha : float, optional\n",
    "        Initial learning rate (step size) in (0, 1]. Default is 0.1.\n",
    "    alpha_factor : float, optional\n",
    "        Multiplicative decay applied to ``alpha`` after each episode. If None, no decay is\n",
    "        applied. Default is 0.9995.\n",
    "    gamma : float, optional\n",
    "        Discount factor in [0, 1]. Default is 0.99.\n",
    "    epsilon : float, optional\n",
    "        Exploration probability for the epsilon-greedy behavior policy. Default is 0.5.\n",
    "    num_episodes : int, optional\n",
    "        Number of training episodes. Default is 10000.\n",
    "    display : bool, optional\n",
    "        If True, periodically visualizes the current Q-table every\n",
    "        ``DISPLAY_EVERY_N_EPISODES`` episodes. Default is False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    q_array : np.ndarray\n",
    "        Learned Q-table of shape (num_states, num_actions).\n",
    "        For instance, for the FrozenLake environment, q_array[0, 3] is\n",
    "        the Q-value (estimation of the cumulative expected reward) for performing\n",
    "        action 3 (\"move up\") in state 0 (the top left square).\n",
    "    q_array_history : List[np.ndarray]\n",
    "        Snapshot of the Q-table taken once per episode (before the episode starts).\n",
    "        Length is ``num_episodes - 1`` due to the episode loop using ``range(1, num_episodes)``.\n",
    "    alpha_history : List[float]\n",
    "        Value of ``alpha`` recorded once per episode (before applying ``alpha_factor``).\n",
    "        Length matches ``q_array_history``.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This implementation assumes the environment follows the Gymnasium step API:\n",
    "    ``new_state, reward, terminated, truncated, info = env.step(action)``.\n",
    "    \"\"\"\n",
    "    # Initialize the history of the Q-table and learning rate\n",
    "    q_array_history = []\n",
    "    alpha_history = []\n",
    "\n",
    "    observation_space = cast(gym.spaces.Discrete, environment.observation_space)\n",
    "    action_space = cast(gym.spaces.Discrete, environment.action_space)\n",
    "\n",
    "    # Get the number of states in the environment\n",
    "    num_states = observation_space.n\n",
    "\n",
    "    # Get the number of actions in the environment\n",
    "    num_actions = action_space.n\n",
    "\n",
    "    # Initialize the Q-table to zeros\n",
    "    q_array = np.zeros([num_states, num_actions])\n",
    "\n",
    "    # Loop over the episodes\n",
    "    for episode_index in tqdm(range(1, num_episodes)):\n",
    "        # Display the Q-table every DISPLAY_EVERY_N_EPISODES episodes if display is True\n",
    "        if display and episode_index % DISPLAY_EVERY_N_EPISODES == 0:\n",
    "            display_qtable(q_array, title=\"Q table\")\n",
    "\n",
    "        # Save the current Q-table and learning rate\n",
    "        q_array_history.append(q_array.copy())\n",
    "        alpha_history.append(alpha)\n",
    "\n",
    "        # Decrease the learning rate if alpha_factor is not None\n",
    "        if alpha_factor is not None:\n",
    "            alpha = alpha * alpha_factor\n",
    "\n",
    "        # TODO ðŸ‘ˆ Complete the implementation of the Q-learning algorithm\n",
    "\n",
    "    # Return the learned Q-table\n",
    "    return q_array, q_array_history, alpha_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47785fde",
   "metadata": {},
   "source": [
    "**Note**: In the following cell, the `display` argument can be set to `True` to see the evolution of the action-value function `q_array` over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f865981",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create the environment and set the maximum number of steps per episode\n",
    "environment = gym.make(\"FrozenLake-v1\", max_episode_steps=1000)\n",
    "\n",
    "# Apply Q-learning to calculate the Q-table for the FrozenLake environment\n",
    "q_array_ex3, q_array_history_ex3, alpha_history_ex3 = q_learning(environment, display=False)\n",
    "\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c82b69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the learned Q-table\n",
    "display_qtable(q_array_ex3, title=\"Q Table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959eb0ee",
   "metadata": {},
   "source": [
    "### Display the greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2ef2e7-5173-4c94-95b9-84049228c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_ex3 = [greedy_policy(state, q_array_ex3) for state in range(environment.observation_space.n)]\n",
    "policy_ex3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaa3ffe-352b-4594-baa8-3113b6c6ecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_policy(policy_ex3, environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b48d5b-cca9-4a46-9ebb-f3430e66d786",
   "metadata": {},
   "source": [
    "### Display the evolution of the value function over iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcca3096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a Pandas dataframe containing the evolution of the Q-table during the learning process\n",
    "q_array_history_array_ex3 = np.array(q_array_history_ex3)\n",
    "df_q_hist_list_ex3 = []\n",
    "\n",
    "for action_index in range(q_array_history_array_ex3.shape[2]):\n",
    "    df_q_hist_list_ex3.append(pd.DataFrame(q_array_history_array_ex3[:, :, action_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928f2a34",
   "metadata": {},
   "source": [
    "Evolution of `q_array_ex3` over iterations (one curve per state):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07469c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the evolution of the estimated expected value for each action and state during the learning process\n",
    "for action_index, df_q_hist in enumerate(df_q_hist_list_ex3):\n",
    "    df_q_hist.plot(figsize=(14,8))\n",
    "    plt.title(r'$Q(\\cdot,a_{})$ w.r.t iteration with $a_{}$ := \"{}\"'.format(action_index, action_index, action_labels[action_index]))\n",
    "    plt.ylabel(r\"$Q(\\cdot,a_{})$\".format(action_index))\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2fa97a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Show the evolution of the alpha parameter during the learning process\n",
    "plt.loglog(alpha_history_ex3)\n",
    "plt.title(\"Alpha w.r.t iteration\")\n",
    "plt.ylabel(\"Alpha\")\n",
    "plt.xlabel(\"iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca26609",
   "metadata": {},
   "source": [
    "### Evaluate Policy with Gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fba3e6",
   "metadata": {},
   "source": [
    "As a measure of performance, count the number of successful trials on 1000 episodes.\n",
    "\n",
    "**Note**: Gymnasium considers the task is solved if you reach 76\\% of success over the episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74a1c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = gym.make(\"FrozenLake-v1\", max_episode_steps=1000)\n",
    "\n",
    "reward_list = []\n",
    "NUM_EPISODES = 1000\n",
    "\n",
    "for episode_index in range(NUM_EPISODES):\n",
    "    state, info = environment.reset()\n",
    "    episode_over = False\n",
    "\n",
    "    while not episode_over:\n",
    "        # action = epsilon_greedy_policy(state, q_array_ex3, epsilon)\n",
    "        action = greedy_policy(state, q_array_ex3)\n",
    "        state, reward, terminated, truncated, info = environment.step(action)\n",
    "        episode_over = terminated or truncated\n",
    "\n",
    "    reward_list.append(reward)\n",
    "\n",
    "reward_df = pd.DataFrame(reward_list)\n",
    "\n",
    "print(\n",
    "    'Average reward (which is equivalent to a \"success rate\" in the FrozenLake environment as the total rewards in this environment are either 0 or 1):',\n",
    "    np.average(reward_df),\n",
    ")\n",
    "\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88f9aaf-7487-4419-9dc1-3c977ee932b4",
   "metadata": {},
   "source": [
    "### Test the optimized policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6250b420-4ab6-4f46-a048-918768739579",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PREFIX_EX3 = \"lab4_ex3_qlearning\"\n",
    "\n",
    "NUM_EPISODES = 3\n",
    "\n",
    "file_path_list = [\n",
    "    FIGS_DIR / f\"{VIDEO_PREFIX_EX3}-episode-{episode_index}.mp4\"\n",
    "    for episode_index in range(NUM_EPISODES)\n",
    "]\n",
    "\n",
    "for file_path in file_path_list:\n",
    "    file_path.unlink(missing_ok=True)\n",
    "\n",
    "environment = gym.make(\"FrozenLake-v1\", max_episode_steps=1000, render_mode=\"rgb_array\")\n",
    "environment = gym.wrappers.RecordVideo(\n",
    "    environment,\n",
    "    video_folder=str(FIGS_DIR),\n",
    "    name_prefix=VIDEO_PREFIX_EX3,\n",
    "    episode_trigger=lambda x: True\n",
    ")\n",
    "environment = gym.wrappers.RecordEpisodeStatistics(environment, buffer_length=NUM_EPISODES)\n",
    "\n",
    "for episode_index in range(NUM_EPISODES):\n",
    "    state, info = environment.reset()\n",
    "\n",
    "    episode_over = False\n",
    "    while not episode_over:\n",
    "        # action = epsilon_greedy_policy(state, q_array_ex3, epsilon)\n",
    "        action = greedy_policy(state, q_array_ex3)\n",
    "        state, reward, terminated, truncated, info = environment.step(action)\n",
    "        episode_over = terminated or truncated\n",
    "\n",
    "    reward_list.append(reward)\n",
    "\n",
    "print(f'Episode time taken: {environment.time_queue}')\n",
    "print(f'Episode total rewards: {environment.return_queue}')\n",
    "print(f'Episode lengths: {environment.length_queue}')\n",
    "\n",
    "environment.close()\n",
    "\n",
    "print(\"\\nSelect the episode to play here ðŸ‘‡\\n\")\n",
    "\n",
    "interact(video_selector, file_path=file_path_list);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdbd47e-734b-4bf6-acfe-da53a8649a5b",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11b27ad-ea51-4b43-b54f-9cd5def05ebc",
   "metadata": {},
   "source": [
    "Copy and paste the output of the following cell into the third question of the [Lab 4 - Evaluation](https://moodle.ip-paris.fr/mod/quiz/view.php?id=185078) in Moodle:  \n",
    "*\"What is the Q-value Q(s0,a0) of the initial state and action 0 (\"Move Left\") in the FrozenLake environment using the default parameters?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa8942f-381d-48e0-82ce-c815e774a9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "NUM_RUNS = 10\n",
    "\n",
    "# Create the environment and set the maximum number of steps per episode\n",
    "environment = gym.make(\"FrozenLake-v1\", max_episode_steps=1000)\n",
    "\n",
    "run_list = []\n",
    "for _ in tqdm(range(1, NUM_RUNS)):\n",
    "    q_array, _, _ = q_learning(environment, display=False)\n",
    "    run_list.append(q_array[0][0].item())\n",
    "\n",
    "environment.close()\n",
    "\n",
    "print(f\"\\n\\nâš ï¸ Copy and paste this value into Moodle ðŸ‘‰ {np.mean(run_list).item():.2f}\\n(it has the required number of decimals)\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddc0b00-1638-4c5b-bd81-412206cddfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.std(run_list))\n",
    "#Â pd.DataFrame(run_list).plot.hist();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
